{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning\n",
    "\n",
    "To test the learning process of the actor architecture (that is more complicated than the one of the critic), we can generate a dataset of states of the minigame (e.g. MoveToBeacon) and associate to each state one of the optimal actions that the agent should learn.\n",
    "\n",
    "More in particular, since we already know that the agent is able to select the right action (usually move screen), but not the parameters (where to click), the best thing to to would be to use the Mean Square Error loss between the probabilities from which to sample an action and the optimal probability distribution (assuming equal probability for all optimal choices for example). Since we need to click on the position of the screen where the beacon is (and that's a 3x3 spot in a 16x16 grid), the whole procedure could be very easy if we hadn't to process the whole state for choosing an action: in fact a simple identity from the input layer representing the beacon to the output would be optimal (after that there is a softmax for normalization)!   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "beacon_x, player_x = np.random.choice(14, size=2) + 1 \n",
    "beacon_y, player_y = np.random.choice(14, size=2) + 1\n",
    "\n",
    "state = np.zeros((3,16,16)).astype('float')\n",
    "\n",
    "beacon_layer = np.zeros((16,16))\n",
    "xs = [beacon_x-1, beacon_x, beacon_x + 1]\n",
    "ys = [beacon_x-1, beacon_x, beacon_x + 1]\n",
    "for x in xs:\n",
    "    for y in ys:\n",
    "        beacon_layer[x, y] = 1.\n",
    "        \n",
    "player_layer = np.zeros((16,16))\n",
    "player_layer[player_x, player_y] = 1.\n",
    "\n",
    "# suppose it's always selected, to mimic real input state \n",
    "selected_layer = np.zeros((16,16))\n",
    "selected_layer[player_x, player_y] = 1.\n",
    "\n",
    "state[0] = beacon_layer\n",
    "state[1] = player_layer\n",
    "state[2] = selected_layer\n",
    "\n",
    "## select optimal action distribution ##\n",
    "action_target_distr = beacon_layer/beacon_layer.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beacon pos;  10 1\n",
      "beacon layer: \n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "state; \n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "optimal action distribution: \n",
      " [[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.11111111 0.11111111 0.11111111\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.11111111 0.11111111 0.11111111\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.11111111 0.11111111 0.11111111\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"beacon pos; \", beacon_x, beacon_y)\n",
    "print(\"beacon layer: \\n\", beacon_layer)\n",
    "print(\"state; \\n\", state)\n",
    "print(\"optimal action distribution: \\n\", action_target_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_state():\n",
    "    # sample in (1,14), leave out borders\n",
    "    beacon_x, player_x = np.random.choice(14, size=2) + 1 \n",
    "    beacon_y, player_y = np.random.choice(14, size=2) + 1\n",
    "    \n",
    "    state = np.zeros((3,16,16)).astype(float)\n",
    "    beacon_layer = np.zeros((16,16))\n",
    "    xs = [beacon_x-1, beacon_x, beacon_x + 1]\n",
    "    ys = [beacon_x-1, beacon_x, beacon_x + 1]\n",
    "    for x in xs:\n",
    "        for y in ys:\n",
    "            beacon_layer[x, y] = 1.\n",
    "\n",
    "    player_layer = np.zeros((16,16))\n",
    "    player_layer[player_x, player_y] = 1.\n",
    "\n",
    "    # suppose it's always selected, to mimic real input state \n",
    "    selected_layer = np.zeros((16,16))\n",
    "    selected_layer[player_x, player_y] = 1.\n",
    "\n",
    "    state[0] = beacon_layer\n",
    "    state[1] = player_layer\n",
    "    state[2] = selected_layer\n",
    "\n",
    "    ## select optimal action distribution ##\n",
    "    action_target_distr = beacon_layer/beacon_layer.sum()\n",
    "    \n",
    "    return state, action_target_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_action_state_set(size = 10000):\n",
    "    state_memory = []\n",
    "    action_memory = []\n",
    "    \n",
    "    for i in range(size):\n",
    "        state, action = generate_state()\n",
    "        state_memory.append(state)\n",
    "        action_memory.append(action)\n",
    "        \n",
    "    return np.array(state_memory), np.array(action_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 614 ms, sys: 19.7 ms, total: 634 ms\n",
      "Wall time: 632 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "state_set, action_set = create_action_state_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3, 16, 16)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 16, 16)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Add description\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index], self.label[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(x, label, train_perc, val_perc, train_batch_size, val_batch_size, test_batch_size):\n",
    "    \"\"\"\n",
    "    Add description\n",
    "    \"\"\"\n",
    "    \n",
    "    # training/test splitting\n",
    "    m = int(len(x)*train_perc)\n",
    "    x_train= x[:m]\n",
    "    y_train = label[:m]\n",
    "    x_test =  x[m:]\n",
    "    y_test = label[m:]\n",
    "    \n",
    "    # define custom NumpyDatasets\n",
    "    train_set = NumpyDataset(x_train, y_train)\n",
    "    test_set =  NumpyDataset(x_test, y_test)\n",
    "   \n",
    "    train_len = int(m*(1-val_perc))\n",
    "    train_sampler = SubsetRandomSampler(np.arange(train_len))\n",
    "    val_sampler = SubsetRandomSampler(np.arange(train_len,m))\n",
    "\n",
    "    train_loader = DataLoader(train_set, train_batch_size, sampler=train_sampler, drop_last=True, collate_fn=lambda x: x)\n",
    "    val_loader = DataLoader(train_set, val_batch_size, sampler=val_sampler, drop_last=True, collate_fn=lambda x: x)\n",
    "    test_loader = DataLoader(test_set, test_batch_size, drop_last=False, collate_fn=lambda x: x)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = {'train_batch_size':64, 'val_batch_size':64, 'test_batch_size':128}\n",
    "train_loader, val_loader, test_loader = prepare_dataset(state_set, action_set, 0.8, 0.2, **batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define agents\n",
    "\n",
    "Here we suppose that the actor always chooses the _MOVE_SCREEN action and we only train the network to learn the correct distribution of the spatial parameters.\n",
    "\n",
    "I consider 2 different networks, one that loses the spatial dimension of the input and has to recreate it (Agent 3) and one that keeps that spatial dimension intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConvolutional(nn.Module):\n",
    "    \n",
    "    def __init__(self, linear_size, n_channels, hidden_channels=12, kernel_size=3):\n",
    "        super(ResidualConvolutional, self).__init__()\n",
    "        \n",
    "        padding = (kernel_size - 1) // 2\n",
    "        assert (kernel_size - 1) % 2 == 0, 'Provide odd kernel size to use this layer'\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "                                nn.LayerNorm((linear_size, linear_size)),\n",
    "                                nn.Conv2d(n_channels, hidden_channels, kernel_size, stride=1, padding=padding),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv2d(hidden_channels, n_channels, kernel_size, stride=1, padding=padding)\n",
    "                                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        out = out + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 16, 16])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_size = 16\n",
    "n_channels = 3\n",
    "hidden_channels = 12\n",
    "kernel_size = 5\n",
    "\n",
    "res_conv = ResidualConvolutional(linear_size, n_channels, hidden_channels, kernel_size)\n",
    "x = torch.rand(1,k_in,linear_size,linear_size)\n",
    "y = res_conv(x)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialNet(nn.Module):\n",
    "    def __init__(self, n_layers, linear_size, in_channels, n_channels, **HPs):\n",
    "        super(SpatialNet, self).__init__()\n",
    "        \n",
    "        self.linear_size = linear_size\n",
    "        \n",
    "        layers =  nn.ModuleList([ResidualConvolutional(linear_size, n_channels, **HPs) for _ in range(n_layers-1)])\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "                                nn.Conv2d(in_channels, n_channels, kernel_size=3, stride=1, padding=1),\n",
    "                                nn.ReLU(),\n",
    "                                *layers,\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv2d(n_channels, 1, kernel_size=3, stride=1, padding=1)\n",
    "                                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = x.reshape((x.shape[0],-1))\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        x = x.reshape((x.shape[0], self.linear_size, self.linear_size))\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 2\n",
    "linear_size = 16\n",
    "in_channels = 3\n",
    "n_channels = 12\n",
    "\n",
    "spatial_net = SpatialNet(n_layers, linear_size, in_channels, n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(5.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5, in_channels, linear_size, linear_size)\n",
    "y = spatial_net(x)\n",
    "print(y.shape)\n",
    "y.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing function\n",
    "def test_epoch(net, dataloader, loss_fn, optimizer):\n",
    "\n",
    "    # select device\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        \n",
    "    # Validation\n",
    "    net.eval() # Evaluation mode (e.g. disable dropout)\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        batch_len = np.zeros(len(dataloader))\n",
    "        batch_loss = np.zeros(len(dataloader))\n",
    "        for i, data in enumerate(dataloader,0):\n",
    "            # Extract data and move tensors to the selected device\n",
    "            x = [x[0] for x in data]\n",
    "            x = torch.tensor(x).float().to(device)\n",
    "            \n",
    "            y =  [x[1] for x in data]\n",
    "            y = torch.tensor(y).float().to(device)\n",
    "\n",
    "            y_pred = net(x)\n",
    "\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            \n",
    "            # save MSE loss and length of a batch\n",
    "            batch_len[i] = len(data)\n",
    "            batch_loss[i] = loss.item()\n",
    "    \n",
    "    # total loss\n",
    "    val_loss = (batch_loss*batch_len).sum()/batch_len.sum()\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NN(net, lr, n_epochs, train_loader, val_loader, train_log=True, verbose=True, \n",
    "                  debug=False, return_model = False):\n",
    "    \"\"\"\n",
    "    Trains a Pytorch model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Pytorch nn.Module class \n",
    "        Must have forward method\n",
    "    train_loader: torch DataLoader\n",
    "        Loads the training set\n",
    "    val_loader: torch DataLoader\n",
    "        Loads the validation set\n",
    "    verbose: bool\n",
    "        If True prints updates of the training 10 times for each epoch\n",
    "    return_model: bool\n",
    "        If True returns the trained instance of the model \n",
    "    **params: dictionary \n",
    "        Must contain all the parameters needed by the model, the optimizer and the loss\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    net (if return_model): Pytorch nn.Module class\n",
    "        Trained instance of the model \n",
    "    train_loss_log (if train_log): list\n",
    "        Training loss for each epoch\n",
    "    val_loss_log (if train_log): list\n",
    "        Validation loss for each epoch\n",
    "    val_acc_log (if train_log): list\n",
    "        Validation accuracy for each epoch\n",
    "    \n",
    "    \"\"\"\n",
    "  \n",
    "    optimizer = optim.Adamax(net.parameters(), lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # define contextual print functions activated by print flags\n",
    "    verbose_print = print if verbose else lambda *a, **k: None\n",
    "    verbose_print(\"Verbose: \", verbose)\n",
    "    dprint = print if debug else lambda *a, **k: None\n",
    "    dprint(\"Debug: \", debug)\n",
    "\n",
    "    # If cuda is available set the device to GPU\n",
    "    verbose_print(\"Using cuda: \", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    # Move all the network parameters to the selected device (if they are already on that device nothing happens)\n",
    "    net.to(device)\n",
    "    \n",
    "    n_batches = len(train_loader)\n",
    "    epoch_time = []\n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    # lists with the history of the training\n",
    "    if (train_log == True):\n",
    "        train_loss_log = []\n",
    "        val_loss_log = []\n",
    "\n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches // 10 # frequency of printing\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        batches_done = 0\n",
    "        net.train() # activate dropout\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            batches_done += 1\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x = [x[0] for x in data]\n",
    "            x = torch.tensor(x).float().to(device)\n",
    "            \n",
    "            y =  [x[1] for x in data]\n",
    "            y = torch.tensor(y).float().to(device)\n",
    "\n",
    "            y_pred = net(x)\n",
    "\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #Print statistics\n",
    "            running_loss += loss.item() \n",
    "            total_train_loss += loss.item()\n",
    "            #Print every 10th batch of an epoch\n",
    "            if ((i+1) % (print_every) == 0) or (i == n_batches - 1):\n",
    "                verbose_print('\\r'+\"Epoch {}, {:d}% \\t Train loss: {:.4f} took: {:.2f}s \".format(\n",
    "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / batches_done,\n",
    "                        time.time() - start_time), end=' ')\n",
    "                \n",
    "        epoch_time.append(time.time() - start_time)\n",
    "        if (train_log == True):\n",
    "            train_loss_log.append(total_train_loss/len(train_loader))\n",
    "        \n",
    "        \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        val_loss = test_epoch(net, dataloader=val_loader, loss_fn=loss_fn, optimizer=optimizer) \n",
    "        if (train_log == True):\n",
    "            val_loss_log.append(val_loss)\n",
    "            verbose_print(\"Val. loss: {:.4f}\".format(val_loss ))\n",
    "\n",
    "    verbose_print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))\n",
    "    if train_log:\n",
    "        if return_model:\n",
    "            return net, train_loss_log, val_loss_log#, val_acc_log\n",
    "        else:\n",
    "            return train_loss_log, val_loss_log#, val_acc_log  #used during cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "n_epochs = 5\n",
    "\n",
    "n_layers = 2\n",
    "linear_size = 16\n",
    "in_channels = 3\n",
    "n_channels = 12\n",
    "\n",
    "spatial_net = SpatialNet(n_layers, linear_size, in_channels, n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbose:  True\n",
      "Using cuda:  True\n",
      "Epoch 1, 100% \t Train loss: 0.0002 took: 1.72s  Val. loss: 0.0000\n",
      "Epoch 2, 100% \t Train loss: 0.0000 took: 1.34s  Val. loss: 0.0000\n",
      "Epoch 3, 100% \t Train loss: 0.0000 took: 1.28s  Val. loss: 0.0000\n",
      "Epoch 4, 100% \t Train loss: 0.0000 took: 1.56s  Val. loss: 0.0000\n",
      "Epoch 5, 100% \t Train loss: 0.0000 took: 1.33s  Val. loss: 0.0000\n",
      "Training finished, took 8.38s\n"
     ]
    }
   ],
   "source": [
    "trained_spatial_net, spatial_train_loss, spatial_val_loss = train_NN(spatial_net, lr, n_epochs, train_loader, \n",
    "                                                                     val_loader, return_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = generate_state()\n",
    "x = torch.tensor(state).float().unsqueeze(0).to('cuda')\n",
    "probs = spatial_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD4CAYAAAAjDTByAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAANSUlEQVR4nO3df+xd9V3H8eeLloIwBkUcMCADFkKCyxTSINsMLlawIKGY7I8Sp3XMkEVRUJetC4ks+o9zOp26bEFAUQksY+DIBo6GbVlMpFup5dfKoCCDQkeZLIDDUQpv/7in5vbL99t+v/eee/nWz/OR3Nxz7/mc73n3c++r59xzzz2fVBWS2nPAG12ApDeG4ZcaZfilRhl+qVGGX2rU0mmubFkOqoM5dJqrlJryY37Ezno582k71fAfzKH8XFZOc5VSUzbUXfNu626/1CjDLzVqrPAnWZXku0m2JlnXV1GSJm/k8CdZAnwGOA84Dbg4yWl9FSZpssbZ8p8JbK2qx6pqJ3ATsLqfsiRN2jjhPw54cujxtu65PSS5NMnGJBtf4eUxViepT+OEf7bvEl/3E8GqurqqVlTVigM5aIzVSerTOOHfBpww9Ph44OnxypE0LeOE/9vAKUlOSrIMWAPc1k9ZkiZt5DP8qmpXksuArwJLgOuq6sHeKpM0UWOd3ltVtwO391SLpCnyDD+pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfatQ4I/ackOTrSbYkeTDJ5X0WJmmyxrmG3y7gD6tqU5LDgHuSrK+q7/RUm6QJGnnLX1Xbq2pTN/0isIVZRuyRtDiNdfXe3ZKcCJwObJhl3qXApQAHc0gfq5PUg7EP+CV5E/BF4IqqemHmfIfrkhanscKf5EAGwb+hqm7ppyRJ0zDO0f4A1wJbqupT/ZUkaRrG2fK/B/h14BeTbO5u5/dUl6QJG2esvn9j9mG6Je0HPMNPapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxrVx6W7lyT5jyRf7qMgSdPRx5b/cgaj9Ujaj4x73f7jgV8BrumnHEnTMu6W/6+AjwCv9VCLpCkaZ9COC4AdVXXPPtpdmmRjko2v8PKoq5PUs3EH7bgwyePATQwG7/jnmY0cq09anMYZovtjVXV8VZ0IrAG+VlXv760ySRPl9/xSo0YermtYVX0D+EYff0vSdLjllxrVy5Z/QTLC2J5V/dchNc4tv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9SocUfsOSLJzUkeSrIlybv6KkzSZI17Ga9PA/9aVe9Lsgw4pIeaJE3ByOFP8mbgbOA3AapqJ7Czn7IkTdo4u/0nA88Cf98N0X1NkkNnNnK4LmlxGif8S4EzgM9W1enAj4B1Mxs5XJe0OI0T/m3Atqra0D2+mcF/BpL2A+OM1fd94Mkkp3ZPrQS+00tVkiZu3KP9vwvc0B3pfwz4wPglSZqGscJfVZuBFT3VImmKPMNPapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxo17u/5F65q6quU9Hpu+aVGGX6pUYZfatS4w3X9fpIHkzyQ5MYkB/dVmKTJGjn8SY4Dfg9YUVXvAJYAa/oqTNJkjbvbvxT4iSRLGYzT9/T4JUmahnGu2/8U8OfAE8B24PmqunNmO4frkhancXb7lwOrgZOAtwKHJnn/zHYO1yUtTuPs9v8S8J9V9WxVvQLcAry7n7IkTdo44X8COCvJIUnCYLiuLf2UJWnSxvnMv4HB4JybgPu7v3V1T3VJmrBxh+u6Criqp1okTZFn+EmNMvxSowy/1CjDLzXK8EuNMvxSowy/1CjDLzXK8EuNMvxSowy/1CjDLzXK8EuNMvxSowy/1CjDLzXK8EuNMvxSo/YZ/iTXJdmR5IGh545Msj7JI9398smWKalv89ny/wOwasZz64C7quoU4K7usaT9yD7DX1XfBJ6b8fRq4Ppu+nrgop7rkjRho37mP7qqtgN092+Zq6HDdUmL08QP+Dlcl7Q4jRr+Z5IcC9Dd7+ivJEnTMGr4bwPWdtNrgS/1U46kaZnPV303Av8OnJpkW5IPAn8KnJPkEeCc7rGk/cg+h+uqqovnmLWy51okTZFn+EmNGmugTqk5yRtdwd7V/Ju65ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qUP+xRk7J0tLf+AUccPtJy9T8/Hmm5hcpL89+eu+WXGmX4pUYZfqlRow7X9ckkDyW5L8mtSY6YbJmS+jbqcF3rgXdU1TuBh4GP9VyXpAkbabiuqrqzqnZ1D+8Gjp9AbZImqI/P/JcAd8w10+G6pMVprPAnuRLYBdwwVxuH65IWp5FP8kmyFrgAWFlVC7hmqKTFYKTwJ1kFfBT4hap6qd+SJE3DqMN1/S1wGLA+yeYkn5twnZJ6NupwXddOoBZJU+QZflKj/FWf9n8HLFnwIkuOOXqkVX3lW18Zabl7Xt654GWe2rXwE2cfvWj+h+Dc8kuNMvxSowy/1CjDLzXK8EuNMvxSowy/1CjDLzXK8EuNMvxSowy/1CjDLzXK8EuN8ld92v/Va1Nb1Q9fHe3CVW9buvAr3b1z2YsLXuZPDnh13m3d8kuNMvxSo0Yarmto3oeTVJKjJlOepEkZdbgukpwAnAM80XNNkqZgpOG6On8JfATwmv3Sfmikz/xJLgSeqqp759HW4bqkRWjBX/UlOQS4Ejh3Pu2r6mrgaoA350j3EqRFYpQt/9uBk4B7kzzOYITeTUmO6bMwSZO14C1/Vd0PvGX34+4/gBVV9YMe65I0YaMO1yVpPzfqcF3D80/srRpJU+MZflKj/GGP9n+18C+RXvvBf420qhVf+IORlrt29dULXuaqK35rwcs8/L1Pz7utW36pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUakRfhE18sqSZ4HvzTH7KGAxXA3IOvZkHXta7HW8rap+aj5/YKrh35skG6tqhXVYh3VMpw53+6VGGX6pUYsp/Au/1MlkWMeerGNP/2/qWDSf+SVN12La8kuaIsMvNWqq4U+yKsl3k2xNsm6W+Qcl+Xw3f0OSEydQwwlJvp5kS5IHk1w+S5v3Jnk+yebu9kd91zG0rseT3N+tZ+Ms85Pkr7s+uS/JGT2v/9Shf+fmJC8kuWJGm4n1R5LrkuxI8sDQc0cmWZ/kke5++RzLru3aPJJk7QTq+GSSh7p+vzXJEXMsu9fXsIc6Pp7kqaH+P3+OZfear9epqqncgCXAo8DJwDLgXuC0GW1+G/hcN70G+PwE6jgWOKObPgx4eJY63gt8eUr98jhw1F7mnw/cAQQ4C9gw4dfo+wxOFJlKfwBnA2cADww992fAum56HfCJWZY7Enisu1/eTS/vuY5zgaXd9Cdmq2M+r2EPdXwc+PA8Xru95mvmbZpb/jOBrVX1WFXtBG4CVs9osxq4vpu+GViZJH0WUVXbq2pTN/0isAU4rs919Gw18I81cDdwRJJjJ7SulcCjVTXXWZi9q6pvAs/NeHr4fXA9cNEsi/4ysL6qnquqHwLrgVV91lFVd1bVru7h3QwGpZ2oOfpjPuaTrz1MM/zHAU8OPd7G60P3f226Tn8e+MlJFdR9rDgd2DDL7HcluTfJHUl+elI1AAXcmeSeJJfOMn8+/daXNcCNc8ybVn8AHF1V22HwnzVDA8MOmWa/AFzCYA9sNvt6DftwWffx47o5PgYtuD+mGf7ZtuAzv2ecT5teJHkT8EXgiqp6YcbsTQx2fX8G+BvgXyZRQ+c9VXUGcB7wO0nOnlnqLMv03idJlgEXAl+YZfY0+2O+pvleuRLYBdwwR5N9vYbj+izwduBnge3AX8xW5izP7bU/phn+bcAJQ4+PB56eq02SpcDhjLYLtFdJDmQQ/Buq6paZ86vqhar67276duDAJEf1XUf395/u7ncAtzLYfRs2n37rw3nApqp6ZpYap9YfnWd2f7Tp7nfM0mYq/dIdSLwA+LXqPlzPNI/XcCxV9UxVvVpVrwF/N8ffX3B/TDP83wZOSXJSt5VZA9w2o81twO6jtu8DvjZXh4+qO4ZwLbClqj41R5tjdh9rSHImg34abXC3vddyaJLDdk8zOMD0wIxmtwG/0R31Pwt4fvcucc8uZo5d/mn1x5Dh98Fa4EuztPkqcG6S5d1u8Lndc71Jsgr4KHBhVb00R5v5vIbj1jF8jOdX5/j788nXnvo4QrmAI5nnMzi6/ihwZffcHzPoXICDGex2bgW+BZw8gRp+nsHu0H3A5u52PvAh4ENdm8uABxkcMb0bePeE+uPkbh33duvb3SfDtQT4TNdn9wMrJlDHIQzCfPjQc1PpDwb/4WwHXmGw9fogg+M8dwGPdPdHdm1XANcMLXtJ917ZCnxgAnVsZfA5evf7ZPc3UW8Fbt/ba9hzHf/Uvfb3MQj0sTPrmCtfe7t5eq/UKM/wkxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUf8LEsLNcGRyEvUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(state[0])\n",
    "plt.show()\n",
    "plt.imshow(probs.cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is so simple that is learned almost immediately in a spatial net with residual connections. Actually as I said above a single linear layer would be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
