{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I do:\n",
    "- Init CUDA\n",
    "- Load agent on GPU\n",
    "\n",
    "Note:\n",
    "1 MiB = 1 048 576 byte "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "from SC_Utils.game_utils import ObsProcesser, get_action_dict\n",
    "from SC_Utils.train_v2 import *\n",
    "from AC_modules.BatchedA2C import SpatialA2C, ActionDependentA2C\n",
    "import AC_modules.Networks as net\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 16 17:53:25 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.59       Driver Version: 440.59       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 2060    Off  | 00000000:B3:00.0 Off |                  N/A |\r\n",
      "| 20%   32C    P8     6W / 160W |     20MiB /  5932MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1266      G   /usr/lib/xorg/Xorg                             9MiB |\r\n",
      "|    0      1305      G   /usr/bin/gnome-shell                           8MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# start with 20MiB /  5932MiB used\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimal code to initialize CUDA\n",
    "a=torch.cuda.FloatTensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 16 17:53:26 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.59       Driver Version: 440.59       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 2060    Off  | 00000000:B3:00.0 Off |                  N/A |\r\n",
      "| 20%   33C    P2    24W / 160W |    414MiB /  5932MiB |      5%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0       665      C   ...naconda/2020-01-tf2/5a34a04a/bin/python   383MiB |\r\n",
      "|    0      1266      G   /usr/lib/xorg/Xorg                             9MiB |\r\n",
      "|    0      1305      G   /usr/bin/gnome-shell                           8MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# 414MiB / 5932MiB used -> 394 MiB used for CUDA init\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "RESOLUTION = 64\n",
    "MAX_STEPS = 256\n",
    "game_params = dict(feature_screen=RESOLUTION, feature_minimap=RESOLUTION, action_space=\"FEATURES\") \n",
    "game_names = {1:'MoveToBeacon',\n",
    "              2:'CollectMineralShards',\n",
    "              3:'DefeatRoaches',\n",
    "              4:'FindAndDefeatZerglings',\n",
    "              5:'DefeatZerglingsAndBanelings',\n",
    "              6:'CollectMineralsAndGas',\n",
    "              7:'BuildMarines'\n",
    "              }\n",
    "map_name = game_names[2]\n",
    "\n",
    "# Observation Processer parameters\n",
    "screen_names = ['visibility_map', 'player_relative', 'selected', 'unit_density', 'unit_density_aa']\n",
    "minimap_names = []\n",
    "obs_proc_params = {'screen_names':screen_names, 'minimap_names':minimap_names}\n",
    "#obs_proc_params = {'select_all':True}\n",
    "\n",
    "env = init_game(game_params, map_name)\n",
    "op = ObsProcesser(**obs_proc_params)\n",
    "screen_channels, minimap_channels = op.get_n_channels()\n",
    "in_channels = screen_channels + minimap_channels \n",
    "\n",
    "action_names = ['select_point', 'Move_screen']\n",
    "#action_names = ['no_op', 'select_army', 'Attack_screen', 'Move_screen', 'select_point', 'select_rect']\n",
    "action_dict = get_action_dict(action_names)\n",
    "action_space = len(action_dict)\n",
    "\n",
    "spatial_model = net.FullyConvSpatial\n",
    "nonspatial_model = net.FullyConvNonSpatial\n",
    "embed_dim = 8\n",
    "n_channels = 32\n",
    "n_features = 256\n",
    "spatial_dict = {\"in_channels\":in_channels}\n",
    "nonspatial_dict = {'resolution':RESOLUTION, 'kernel_size':3, 'stride':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "HPs = dict(action_space=action_space, gamma=0.99, n_steps=20, H=1e-3, \n",
    "           spatial_model=spatial_model, nonspatial_model=nonspatial_model,\n",
    "           n_features=n_features, n_channels=n_channels, \n",
    "           spatial_dict=spatial_dict, nonspatial_dict=nonspatial_dict, \n",
    "           action_dict=action_dict, embed_dim=embed_dim)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    HPs['device'] = 'cuda'\n",
    "else:\n",
    "    HPs['device'] = 'cpu'\n",
    "    \n",
    "print(\"Using device \"+HPs['device'])\n",
    "\n",
    "lr = 7e-4\n",
    "\n",
    "agent = ActionDependentA2C(env=env, **HPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 16 17:53:35 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.59       Driver Version: 440.59       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 2060    Off  | 00000000:B3:00.0 Off |                  N/A |\r\n",
      "| 20%   33C    P2    25W / 160W |    476MiB /  5932MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0       665      C   ...naconda/2020-01-tf2/5a34a04a/bin/python   445MiB |\r\n",
      "|    0      1266      G   /usr/lib/xorg/Xorg                             9MiB |\r\n",
      "|    0      1305      G   /usr/bin/gnome-shell                           8MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# 430MiB / 5932MiB used -> 16 MiB for loading the agent\n",
    "### 64 x 64 ###\n",
    "# 476MiB /  5932MiB -> 62 MiB\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory in bytes:  63638408\n",
      "Memory in MiB:  60.69031524658203\n"
     ]
    }
   ],
   "source": [
    "byte_memory = 0\n",
    "for p in agent.AC.parameters():\n",
    "    byte_memory += p.element_size()*p.nelement()\n",
    "print(\"Memory in bytes: \", byte_memory)\n",
    "MiB_memory = byte_memory/2**20\n",
    "print(\"Memory in MiB: \", MiB_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14.7 MiB of memory only for the learnable parameters is close enough to the 16 MiB counted by nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unroll_length = 30\n",
    "n_train_processes = 10\n",
    "\n",
    "train_dict = dict(n_train_processes = n_train_processes,\n",
    "                  max_train_steps = unroll_length*10000,\n",
    "                  unroll_length = unroll_length,\n",
    "                  test_interval = unroll_length*50 #100\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = ParallelEnv(n_train_processes, game_params, map_name, obs_proc_params, action_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(agent.AC.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input shape for forward pass: (b, 7, 32, 32)\n",
    "Output shapes: (b,) , scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_idx = 0\n",
    "s_lst, r_lst, done_lst, bootstrap_lst, s_trg_lst = list(), list(), list(), list(), list()\n",
    "log_probs = []\n",
    "entropies = []\n",
    "s, a_mask = envs.reset()\n",
    "for _ in range(unroll_length):\n",
    "\n",
    "    a, log_prob, entropy = agent.step(s, a_mask)\n",
    "    # variables with gradient\n",
    "    log_probs.append(log_prob)\n",
    "    entropies.append(entropy)\n",
    "\n",
    "    s_prime, r, done, bootstrap, s_trg, a_mask = envs.step(a)\n",
    "    s_lst.append(s)\n",
    "    r_lst.append(r)\n",
    "    done_lst.append(done)\n",
    "    bootstrap_lst.append(bootstrap)\n",
    "    s_trg_lst.append(s_trg)\n",
    "    \n",
    "\n",
    "    s = s_prime\n",
    "    step_idx += 1 #n_train_processes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "byte_memory = 0\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            print(type(obj), obj.size())\n",
    "            byte_memory += obj.element_size()*obj.nelement() \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_memory/2**20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated()/2**20 #912.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "785"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4252 - 3463 #??\n",
    "1652 - 867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 16 17:52:20 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.59       Driver Version: 440.59       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 2060    Off  | 00000000:B3:00.0 Off |                  N/A |\r\n",
      "| 20%   33C    P2    24W / 160W |   1706MiB /  5932MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1266      G   /usr/lib/xorg/Xorg                             9MiB |\r\n",
      "|    0      1305      G   /usr/bin/gnome-shell                           8MiB |\r\n",
      "|    0     32569      C   ...naconda/2020-01-tf2/5a34a04a/bin/python  1675MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "### 1 process ###\n",
    "# 784MiB /  5932MiB after 1 forward loop -> 354 MiB used\n",
    "# 786MiB /  5932MiB after 2 forward loop -> 354 + 2MiB used\n",
    "# 868MiB /  5932MiB after 120 forward loops -> 438 MiB used\n",
    "# 1038MiB /  5932MiB if executed a second time after backward\n",
    "# 926MiB /  5932MiB if executed a second time after backward and empty_cache\n",
    "### 10 processes ###\n",
    "# 1652MiB /  5932MiB after 120 forward loops -> 1222 MiB used\n",
    "# 1694MiB /  5932MiB if executed a second time after backward and empty_cache -> 42 MiB more\n",
    "# 1706\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_lst.shape:  (10, 120, 7, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "s_lst = np.array(s_lst).transpose(1,0,2,3,4)\n",
    "print('s_lst.shape: ', s_lst.shape)\n",
    "r_lst = np.array(r_lst).transpose(1,0)\n",
    "done_lst = np.array(done_lst).transpose(1,0)\n",
    "bootstrap_lst = np.array(bootstrap_lst).transpose(1,0)\n",
    "s_trg_lst = np.array(s_trg_lst).transpose(1,0,2,3,4)\n",
    "\n",
    "critic_loss, actor_loss, entropy_term = agent.compute_ac_loss(r_lst, log_probs, entropies, \n",
    "                                                         s_lst, done_lst, bootstrap_lst, s_trg_lst)\n",
    "\n",
    "\n",
    "loss = (critic_loss + actor_loss).mean()\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.896484375"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()/2**20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3296.91943359375"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_allocated()/2**20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "348"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5932 - 5584"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 16 17:52:32 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.59       Driver Version: 440.59       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 2060    Off  | 00000000:B3:00.0 Off |                  N/A |\r\n",
      "| 20%   34C    P2    24W / 160W |   2810MiB /  5932MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1266      G   /usr/lib/xorg/Xorg                             9MiB |\r\n",
      "|    0      1305      G   /usr/bin/gnome-shell                           8MiB |\r\n",
      "|    0     32569      C   ...naconda/2020-01-tf2/5a34a04a/bin/python  2779MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "### 1 process ###\n",
    "# 948MiB /  5932MiB with cudnn backend\n",
    "#1036MiB /  5932MiB after backward with 1 process -> 168 MiB used\n",
    "# 1094MiB /  5932MiB if executed a second time after another forward cycle\n",
    "# 1090MiB /  5932MiB if executed a second time after empty_cache and another forward cycle\n",
    "### 10 processes ###\n",
    "# 3188MiB /  5932MiB after backward with 10 process -> 1536 MiB used\n",
    "# 3230MiB /  5932MiB if executed a second time after empty_cache and another forward cycle\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 16 17:52:36 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.59       Driver Version: 440.59       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 2060    Off  | 00000000:B3:00.0 Off |                  N/A |\r\n",
      "| 20%   34C    P2    24W / 160W |   1110MiB /  5932MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1266      G   /usr/lib/xorg/Xorg                             9MiB |\r\n",
      "|    0      1305      G   /usr/bin/gnome-shell                           8MiB |\r\n",
      "|    0     32569      C   ...naconda/2020-01-tf2/5a34a04a/bin/python  1079MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# 16MiB /  5932MiB  with cudnn\n",
    "# 924MiB /  5932MiB after torch.cuda.empty_cache() - freed 112 MiB\n",
    "# 1080MiB /  5932MiB after torch.cuda.empty_cache() - freed 2108 MiB\n",
    "# 1104\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del critic_loss, actor_loss, entropy_term, loss\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 16 17:27:15 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.59       Driver Version: 440.59       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 2060    Off  | 00000000:B3:00.0 Off |                  N/A |\r\n",
      "| 20%   32C    P8     6W / 160W |    916MiB /  5932MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1266      G   /usr/lib/xorg/Xorg                             9MiB |\r\n",
      "|    0      1305      G   /usr/bin/gnome-shell                           8MiB |\r\n",
      "|    0     31828      C   ...naconda/2020-01-tf2/5a34a04a/bin/python   885MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# 924MiB /  5932MiB after deleating critic_loss, actor_loss, entropy_term, loss \n",
    "# and calling torch.cuda.empty_cache() -> del had no effect\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad() # does not change the amount of GPU used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 16 17:27:17 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.59       Driver Version: 440.59       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 2060    Off  | 00000000:B3:00.0 Off |                  N/A |\r\n",
      "| 20%   33C    P2    24W / 160W |    916MiB /  5932MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1266      G   /usr/lib/xorg/Xorg                             9MiB |\r\n",
      "|    0      1305      G   /usr/bin/gnome-shell                           8MiB |\r\n",
      "|    0     31828      C   ...naconda/2020-01-tf2/5a34a04a/bin/python   885MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355.74951171875"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_allocated()/2**20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function max_memory_allocated in module torch.cuda:\n",
      "\n",
      "max_memory_allocated(device=None)\n",
      "    Returns the maximum GPU memory occupied by tensors in bytes for a given\n",
      "    device.\n",
      "    \n",
      "    By default, this returns the peak allocated memory since the beginning of\n",
      "    this program. :func:`~torch.cuda.reset_max_memory_allocated` can be used to\n",
      "    reset the starting point in tracking this metric. For example, these two\n",
      "    functions can measure the peak allocated memory usage of each iteration in a\n",
      "    training loop.\n",
      "    \n",
      "    Arguments:\n",
      "        device (torch.device or int, optional): selected device. Returns\n",
      "            statistic for the current device, given by :func:`~torch.cuda.current_device`,\n",
      "            if :attr:`device` is ``None`` (default).\n",
      "    \n",
      "    .. note::\n",
      "        See :ref:`cuda-memory-management` for more details about GPU memory\n",
      "        management.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.cuda.max_memory_allocated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batched_A2C(agent, game_params, map_name, lr, n_train_processes, max_train_steps, \n",
    "                      unroll_length, obs_proc_params, action_dict,\n",
    "                      test_interval=100, num_tests=5, inspection_interval=200):\n",
    "    \n",
    "    replay_dict = dict(save_replay_episodes=num_tests,\n",
    "                       replay_dir='Replays/',\n",
    "                       replay_prefix='A2C_'+map_name)\n",
    "    test_env = init_game(game_params, map_name, **replay_dict) # save just test episodes\n",
    "    op = ObsProcesser(**obs_proc_params)\n",
    "    envs = ParallelEnv(n_train_processes, game_params, map_name, obs_proc_params, action_dict)\n",
    "\n",
    "    optimizer = torch.optim.Adam(agent.AC.parameters(), lr=lr)\n",
    "    PID = gen_PID()\n",
    "    print(\"Process ID: \", PID)\n",
    "    score = []\n",
    "    critic_losses = [] \n",
    "    actor_losses = []\n",
    "    entropy_losses = []\n",
    "    \n",
    "    step_idx = 0\n",
    "    while step_idx < max_train_steps:\n",
    "        s_lst, r_lst, done_lst, bootstrap_lst, s_trg_lst = list(), list(), list(), list(), list()\n",
    "        log_probs = []\n",
    "        entropies = []\n",
    "        s, a_mask = envs.reset()\n",
    "        for _ in range(unroll_length):\n",
    "\n",
    "            a, log_prob, entropy = agent.step(s, a_mask)\n",
    "            # variables with gradient\n",
    "            log_probs.append(log_prob)\n",
    "            entropies.append(entropy)\n",
    "\n",
    "            s_prime, r, done, bootstrap, s_trg, a_mask = envs.step(a)\n",
    "            s_lst.append(s)\n",
    "            r_lst.append(r)\n",
    "            done_lst.append(done)\n",
    "            bootstrap_lst.append(bootstrap)\n",
    "            s_trg_lst.append(s_trg)\n",
    "\n",
    "            s = s_prime\n",
    "            step_idx += 1 #n_train_processes\n",
    "\n",
    "        # all variables without gradient\n",
    "        s_lst = np.array(s_lst).transpose(1,0,2,3,4)\n",
    "        r_lst = np.array(r_lst).transpose(1,0)\n",
    "        done_lst = np.array(done_lst).transpose(1,0)\n",
    "        bootstrap_lst = np.array(bootstrap_lst).transpose(1,0)\n",
    "        s_trg_lst = np.array(s_trg_lst).transpose(1,0,2,3,4)\n",
    "\n",
    "        critic_loss, actor_loss, entropy_term = agent.compute_ac_loss(r_lst, log_probs, entropies, \n",
    "                                                                 s_lst, done_lst, bootstrap_lst, s_trg_lst)\n",
    "\n",
    "        \n",
    "        loss = (critic_loss + actor_loss).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        critic_losses.append(critic_loss.item())\n",
    "        actor_losses.append(actor_loss.item())\n",
    "        entropy_losses.append(entropy_term.item())\n",
    "        \n",
    "        \n",
    "        ### Test time ###\n",
    "        if step_idx % test_interval == 0:\n",
    "            avg_score = test(step_idx, agent, test_env, PID, op, action_dict, num_tests)\n",
    "            if inspection and (step_idx%inspection_interval==0):\n",
    "                inspector = inspection_test(step_idx, agent, test_env, PID, op, action_dict)\n",
    "                # save episode for inspection and model weights at that point\n",
    "                save_path = \"../Results/\"+map_name+\"/Checkpoints/\"\n",
    "                inspector.save_dict(path=save_path)\n",
    "                torch.save(agent.AC.state_dict(), save_path+PID+\"_\"+str(step_idx))\n",
    "            score.append(avg_score)\n",
    "    envs.close()\n",
    "    \n",
    "    losses = dict(critic_losses=critic_losses, actor_losses=actor_losses, entropies=entropy_losses)\n",
    "    return score, losses, agent, PID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(torch.cuda.max_memory_allocated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
