{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions as sc_actions\n",
    "from SC_Utils.game_utils import IMPALA_ObsProcesser, FullObsProcesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_game(game_params, map_name='MoveToBeacon', step_multiplier=8, **kwargs):\n",
    "\n",
    "    race = sc2_env.Race(1) # 1 = terran\n",
    "    agent = sc2_env.Agent(race, \"Testv0\") # NamedTuple [race, agent_name]\n",
    "    agent_interface_format = sc2_env.parse_agent_interface_format(**game_params) #AgentInterfaceFormat instance\n",
    "\n",
    "    game_params = dict(map_name=map_name, \n",
    "                       players=[agent], # use a list even for single player\n",
    "                       game_steps_per_episode = 0,\n",
    "                       step_mul = step_multiplier,\n",
    "                       agent_interface_format=[agent_interface_format] # use a list even for single player\n",
    "                       )  \n",
    "    env = sc2_env.SC2Env(**game_params, **kwargs)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Environment parameters\n",
    "RESOLUTION = 32\n",
    "game_params = dict(feature_screen=RESOLUTION, feature_minimap=RESOLUTION, action_space=\"FEATURES\") \n",
    "game_names = ['MoveToBeacon','CollectMineralShards','DefeatRoaches','FindAndDefeatZerglings',\n",
    "              'DefeatZerglingsAndBanelings','CollectMineralsAndGas','BuildMarines']\n",
    "map_name = game_names[1]\n",
    "obs_proc_params = {'select_all':True}\n",
    "op = FullObsProcesser(**obs_proc_params)\n",
    "screen_channels, minimap_channels, in_player = op.get_n_channels()\n",
    "in_channels = screen_channels + minimap_channels \n",
    "\n",
    "\"\"\"\n",
    "# A2C params\n",
    "spatial_model = net.FullyConvPlayerAndSpatial\n",
    "nonspatial_model = net.FullyConvNonSpatial\n",
    "# Internal features, passed inside a dictionary\n",
    "conv_channels = flags.conv_channels #32\n",
    "player_features = flags.player_features #16\n",
    "# Exposed features, passed outside of a dictionary\n",
    "n_channels = conv_channels + player_features #48\n",
    "n_features = flags.n_features #256\n",
    "\n",
    "spatial_dict = {\"in_channels\":in_channels, 'in_player':in_player, \n",
    "                'conv_channels':conv_channels, 'player_features':player_features}\n",
    "nonspatial_dict = {'resolution':RESOLUTION, 'kernel_size':3, 'stride':2, 'n_channels':n_channels}\n",
    "\n",
    "HPs = dict(spatial_model=spatial_model, nonspatial_model=nonspatial_model,\n",
    "       n_features=n_features, n_channels=n_channels, action_names=flags.action_names,\n",
    "       spatial_dict=spatial_dict, nonspatial_dict=nonspatial_dict)\n",
    "game_params['HPs'] = HPs\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = init_game(game_params, map_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action = sc_actions.FunctionCall(actions.FUNCTIONS.no_op.id, [])\n",
    "action = sc_actions.FunctionCall(actions.FUNCTIONS.select_army.id, [[0]]) \n",
    "#action = sc_actions.FunctionCall(actions.FUNCTIONS.Attack_screen.id, [[0],[1,1]])\n",
    "obs = env.step(actions=[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_names = ['no_op', 'move_camera', 'select_point', 'select_rect', 'select_idle_worker', 'select_army', \n",
    "              'Attack_screen','Attack_minimap', 'Build_Barracks_screen', 'Build_CommandCenter_screen',\n",
    "              'Build_Refinery_screen', 'Build_SupplyDepot_screen','Harvest_Gather_SCV_screen', \n",
    "              'Harvest_Return_SCV_quick', 'HoldPosition_quick', 'Move_screen', 'Move_minimap',\n",
    "              'Rally_Workers_screen', 'Rally_Workers_minimap','Train_Marine_quick', 'Train_SCV_quick']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_ids = [sc_actions.FUNCTIONS[a_name].id for a_name in action_names]\n",
    "action_table = np.array([action_ids[i] for i in range(len(action_ids))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMP_op = IMPALA_ObsProcesser(action_table, **obs_proc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Last Action as additional input\n",
    "\n",
    "Obs: All actions that are invalid or equivalent to no-op are not recorded by the environment, so last actions will be [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_action = obs[0].observation['last_actions']\n",
    "print(last_action)\n",
    "if len(last_action) == 0:\n",
    "    last_action = 0\n",
    "else:\n",
    "    last_action = last_action[0]\n",
    "last_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   6   7  12  13  42  44  79  91 268 273 274 331 332 343\n",
      " 344 477 490]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_action_idx = np.where(IMP_op.action_table == last_action)[0][0]\n",
    "print(IMP_op.action_table)\n",
    "IMP_op.action_table[last_action_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need somehow to embed the last action in a meaningful way (I guess that an embedding layer with embedding dim of 10 would do). After that we can simply concatenate player\\_info with last\\_action.\n",
    "\n",
    "The model already has the information about the action space, so we just need to pass the embed\\_dim variable (we can actually keep it constant to 10 for simplicity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Screen / Minimap / Categorical action \n",
    "\n",
    "Task: tile a binary mask to screen and minimap with ones if respectively last action was acting on the screen or on the minimap, with zeros otherwise.\n",
    "\n",
    "How to understand if an action is for screen or minimap? At the moment I just have a spatial vs categorical distinction at the argument level, but nothing screen vs minimap vs other at the main action level.\n",
    "\n",
    "It makes sense to build a look-up table before the beginning of the training to answer this question as fast as possible during runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions = env.action_spec()[0][1]\n",
    "all_arguments = env.action_spec()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_screen(sc_env_action, screen=True):\n",
    "    \"\"\"\n",
    "    Modify this function in a method for some class that has access to the action specs\n",
    "    (could be the wrapped Environment class, with self.env instead of env)\n",
    "    \"\"\"\n",
    "    all_actions = env.action_spec()[0][1]\n",
    "    all_arguments = env.action_spec()[0][0]\n",
    "    \n",
    "    ###\n",
    "    args = all_actions[sc_env_action].args\n",
    "    names = [all_arguments[arg.id].name for arg in args]\n",
    "    if screen:\n",
    "        return np.any(['screen' in n for n in names])\n",
    "    else:\n",
    "        return np.any(['minimap' in n for n in names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_if_screen(last_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screen_mask = list(map(check_if_screen, IMP_op.action_table))\n",
    "screen_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimap_mask = list(map(lambda x: check_if_screen(x, False), IMP_op.action_table))\n",
    "minimap_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict, names = IMP_op.get_state(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['screen_layers', 'minimap_layers', 'player_features'])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 32, 32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['screen_layers'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 32, 32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple access during run time\n",
    "screen_binary_mask = np.array([screen_mask[last_action_idx]])\n",
    "screen_binary_mask2D = np.tile(screen_binary_mask, [1,32,32])\n",
    "state_dict['screen_layers'] = np.concatenate([state_dict['screen_layers'], screen_binary_mask2D])\n",
    "state_dict['screen_layers'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course same thing for minimap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Spatial input processing\n",
    "\n",
    "*Spatially encoded inputs (minimap and screen) are tiled with binary masks denoting\n",
    "whether the previous action constituted a screen- or minimap-related action. These tensors are then fed to\n",
    "independent residual convolutional blocks, each consisting of one convolutional layer (4 × 4 kernels and stride\n",
    "2) followed by a residual block with 2 convolutional layers (3 × 3 kernels and stride 1), which process and\n",
    "downsample the inputs to [8 × 8 × #channels 1 ] outputs. These tensors are concatenated along the depth\n",
    "dimension to form a singular spatial input (inputs 3D ).*\n",
    "\n",
    "Differences with previous implementation: \n",
    "1. First process them, then merge them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConvolutional(nn.Module):\n",
    "    \n",
    "    def __init__(self, res, n_channels, hidden_channels=12, kernel_size=3):\n",
    "        super(ResidualConvolutional, self).__init__()\n",
    "        \n",
    "        padding = (kernel_size - 1) // 2\n",
    "        assert (kernel_size - 1) % 2 == 0, 'Provide odd kernel size to use this layer'\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "                                nn.LayerNorm((res, res)),\n",
    "                                nn.Conv2d(n_channels, hidden_channels, kernel_size, stride=1, padding=padding),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv2d(hidden_channels, n_channels, kernel_size, stride=1, padding=padding)\n",
    "                                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x) + x\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\" At the moment without residual stuff, just for dimensionality check \"\"\"\n",
    "    def __init__(self, res, in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        new_res = (res - kernel_size + 2*padding)//stride + 1\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            ResidualConvolutional(new_res, out_channels, kernel_size=3),\n",
    "            #nn.ReLU(), # not sure about this activation, it would make the identity operation impossible!\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 16\n",
    "in_channels = screen_channels + 1\n",
    "out_channels = 32\n",
    "conv_block = ConvBlock(res, in_channels, out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#screen_tensor = torch.tensor(state_dict['screen_layers']).float().unsqueeze(0)\n",
    "screen_tensor = torch.rand((1,in_channels, res, res))\n",
    "x_screen = conv_block(screen_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 8, 8])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_screen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size = 4\n",
    "padding = 1\n",
    "stride = 2\n",
    "res = 16\n",
    "new_res = (res - kernel_size + 2*padding)//stride + 1\n",
    "new_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically we will have 2 convolutional blocks, one for the minimap and one for the screen. If we want a shortcut of this we can just merge them together and use a single convolutional block, but of course this way might make a better use of the domani knowledge (i.e. don't treat spatial information at two different scales like if it was from the same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Variable dimensionality recap\n",
    "\n",
    "**screen**: tiling binary mask and adding batch dim: (1, screen_channels, res, res) <br>\n",
    "**minimap**: tiling binary mask and adding batch dim: (1, minimap_channels, res, res) <br>\n",
    "**player**:  adding batch dim: (1, in_player) <br>\n",
    "**last_action**:  adding batch dim: (1, 1) <br>\n",
    "\n",
    "### After state encoding:\n",
    "\n",
    "**inputs_3D**: (1, #channels_1, new_res, new_res) <br>\n",
    "with `new_res = (res - kernel_size + 2*padding)//stride + 1` and `#channels_1 = out_channels*2` (default 64 and 32 respectively)\n",
    "\n",
    "**inputs_2D**: (1, in_player+embed_dim) -> (1,128) -> ReLU -> (1, 64) <br>\n",
    "with default value of embed_dim equal to 10.\n",
    "\n",
    "### After memory processing:\n",
    "Note that inputs_3D are used as input to the Conv2D LSTM and not inputs_2D! Also call the output outputs_3D.\n",
    "\n",
    "Conv2D LSTM: kernel size 3x3, stride 1, (padding of 1 to keep resolution constant), #output_channels 96\n",
    "\n",
    "**outputs_3D**: (1, #output_channels, new_res, new_res) (same spatial resolution)\n",
    "\n",
    "### Main processing (control part for the relational processing)\n",
    "\n",
    "Input: outputs_3D\n",
    "\n",
    "2 flows:\n",
    "\n",
    "SPATIAL: 12-layer deep residual model ( 4 blocks of 3 convolutional layers each )\n",
    "   - first: kernel 4x4, stride 1 (?)\n",
    "   - second and third: kernel 3x3, stride 1 \n",
    "   - \"interleaved with ReLU activations and skip-connections\" (I don't know if after every layer or every block; <br> ReLUs make sense after each layer, skip connections after every block maybe) \n",
    "    \n",
    "Since the output should have the same shape of the relational-spatial outputs, i.e. [8, 8, #channels2], and inputs_3D already have that spatial resolution in the paper, I would substitute 8 with a more generic new_res and deduce that the whole spatial architecture has padding so that the resolution remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
