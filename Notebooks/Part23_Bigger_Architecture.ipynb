{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions as sc_actions\n",
    "from SC_Utils.game_utils import IMPALA_ObsProcesser, FullObsProcesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_game(game_params, map_name='MoveToBeacon', step_multiplier=8, **kwargs):\n",
    "\n",
    "    race = sc2_env.Race(1) # 1 = terran\n",
    "    agent = sc2_env.Agent(race, \"Testv0\") # NamedTuple [race, agent_name]\n",
    "    agent_interface_format = sc2_env.parse_agent_interface_format(**game_params) #AgentInterfaceFormat instance\n",
    "\n",
    "    game_params = dict(map_name=map_name, \n",
    "                       players=[agent], # use a list even for single player\n",
    "                       game_steps_per_episode = 0,\n",
    "                       step_mul = step_multiplier,\n",
    "                       agent_interface_format=[agent_interface_format] # use a list even for single player\n",
    "                       )  \n",
    "    env = sc2_env.SC2Env(**game_params, **kwargs)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Environment parameters\n",
    "RESOLUTION = 32\n",
    "game_params = dict(feature_screen=RESOLUTION, feature_minimap=RESOLUTION, action_space=\"FEATURES\") \n",
    "game_names = ['MoveToBeacon','CollectMineralShards','DefeatRoaches','FindAndDefeatZerglings',\n",
    "              'DefeatZerglingsAndBanelings','CollectMineralsAndGas','BuildMarines']\n",
    "map_name = game_names[1]\n",
    "obs_proc_params = {'select_all':True}\n",
    "op = FullObsProcesser(**obs_proc_params)\n",
    "screen_channels, minimap_channels, in_player = op.get_n_channels()\n",
    "in_channels = screen_channels + minimap_channels \n",
    "\n",
    "\"\"\"\n",
    "# A2C params\n",
    "spatial_model = net.FullyConvPlayerAndSpatial\n",
    "nonspatial_model = net.FullyConvNonSpatial\n",
    "# Internal features, passed inside a dictionary\n",
    "conv_channels = flags.conv_channels #32\n",
    "player_features = flags.player_features #16\n",
    "# Exposed features, passed outside of a dictionary\n",
    "n_channels = conv_channels + player_features #48\n",
    "n_features = flags.n_features #256\n",
    "\n",
    "spatial_dict = {\"in_channels\":in_channels, 'in_player':in_player, \n",
    "                'conv_channels':conv_channels, 'player_features':player_features}\n",
    "nonspatial_dict = {'resolution':RESOLUTION, 'kernel_size':3, 'stride':2, 'n_channels':n_channels}\n",
    "\n",
    "HPs = dict(spatial_model=spatial_model, nonspatial_model=nonspatial_model,\n",
    "       n_features=n_features, n_channels=n_channels, action_names=flags.action_names,\n",
    "       spatial_dict=spatial_dict, nonspatial_dict=nonspatial_dict)\n",
    "game_params['HPs'] = HPs\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = init_game(game_params, map_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action = sc_actions.FunctionCall(actions.FUNCTIONS.no_op.id, [])\n",
    "action = sc_actions.FunctionCall(sc_actions.FUNCTIONS.select_army.id, [[0]]) \n",
    "#action = sc_actions.FunctionCall(actions.FUNCTIONS.Attack_screen.id, [[0],[1,1]])\n",
    "obs = env.step(actions=[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_names = ['no_op', 'move_camera', 'select_point', 'select_rect', 'select_idle_worker', 'select_army', \n",
    "              'Attack_screen','Attack_minimap', 'Build_Barracks_screen', 'Build_CommandCenter_screen',\n",
    "              'Build_Refinery_screen', 'Build_SupplyDepot_screen','Harvest_Gather_SCV_screen', \n",
    "              'Harvest_Return_SCV_quick', 'HoldPosition_quick', 'Move_screen', 'Move_minimap',\n",
    "              'Rally_Workers_screen', 'Rally_Workers_minimap','Train_Marine_quick', 'Train_SCV_quick']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_ids = [sc_actions.FUNCTIONS[a_name].id for a_name in action_names]\n",
    "action_table = np.array([action_ids[i] for i in range(len(action_ids))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMP_op = IMPALA_ObsProcesser(action_table, **obs_proc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Last Action as additional input\n",
    "\n",
    "Obs: All actions that are invalid or equivalent to no-op are not recorded by the environment, so last actions will be [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_action = obs[0].observation['last_actions']\n",
    "print(last_action)\n",
    "if len(last_action) == 0:\n",
    "    last_action = 0\n",
    "else:\n",
    "    last_action = last_action[0]\n",
    "last_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   6   7  12  13  42  44  79  91 268 273 274 331 332 343\n",
      " 344 477 490]\n"
     ]
    }
   ],
   "source": [
    "last_action_idx = np.where(IMP_op.action_table == last_action)[0]\n",
    "print(IMP_op.action_table)\n",
    "#IMP_op.action_table[last_action_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_action_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need somehow to embed the last action in a meaningful way (I guess that an embedding layer with embedding dim of 10 would do). After that we can simply concatenate player\\_info with last\\_action.\n",
    "\n",
    "The model already has the information about the action space, so we just need to pass the embed\\_dim variable (we can actually keep it constant to 10 for simplicity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Screen / Minimap / Categorical action \n",
    "\n",
    "Task: tile a binary mask to screen and minimap with ones if respectively last action was acting on the screen or on the minimap, with zeros otherwise.\n",
    "\n",
    "How to understand if an action is for screen or minimap? At the moment I just have a spatial vs categorical distinction at the argument level, but nothing screen vs minimap vs other at the main action level.\n",
    "\n",
    "It makes sense to build a look-up table before the beginning of the training to answer this question as fast as possible during runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions = env.action_spec()[0][1]\n",
    "all_arguments = env.action_spec()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_screen(env, sc_env_action, screen=True):\n",
    "    \"\"\"\n",
    "    Modify this function in a method for some class that has access to the action specs\n",
    "    (could be the wrapped Environment class, with self.env instead of env)\n",
    "    \"\"\"\n",
    "    all_actions = env.action_spec()[0][1]\n",
    "    all_arguments = env.action_spec()[0][0]\n",
    "    \n",
    "    ###\n",
    "    args = all_actions[sc_env_action].args\n",
    "    names = [all_arguments[arg.id].name for arg in args]\n",
    "    if screen:\n",
    "        return np.any(['screen' in n for n in names])\n",
    "    else:\n",
    "        return np.any(['minimap' in n for n in names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_if_screen(env, last_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screen_mask = list(map(lambda x: check_if_screen(env, x, True), IMP_op.action_table))\n",
    "screen_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimap_mask = list(map(lambda x: check_if_screen(env, x, False), IMP_op.action_table))\n",
    "minimap_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict, names = IMP_op.get_state(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['screen_layers', 'minimap_layers', 'player_features'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 32, 32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['screen_layers'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 32, 32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['minimap_layers'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['player_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 32, 32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple access during run time\n",
    "screen_binary_mask = np.array([screen_mask[last_action_idx[0]]])\n",
    "screen_binary_mask2D = np.tile(screen_binary_mask, [1,32,32])\n",
    "state_dict['screen_layers'] = np.concatenate([state_dict['screen_layers'], screen_binary_mask2D])\n",
    "state_dict['screen_layers'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 32, 32]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,*state_dict['screen_layers'].shape[-2:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course same thing for minimap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Spatial input processing\n",
    "\n",
    "*Spatially encoded inputs (minimap and screen) are tiled with binary masks denoting\n",
    "whether the previous action constituted a screen- or minimap-related action. These tensors are then fed to\n",
    "independent residual convolutional blocks, each consisting of one convolutional layer (4 × 4 kernels and stride\n",
    "2) followed by a residual block with 2 convolutional layers (3 × 3 kernels and stride 1), which process and\n",
    "downsample the inputs to [8 × 8 × #channels 1 ] outputs. These tensors are concatenated along the depth\n",
    "dimension to form a singular spatial input (inputs 3D ).*\n",
    "\n",
    "Differences with previous implementation: \n",
    "1. First process them, then merge them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConvLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, res, n_channels, kernel_size=3):\n",
    "        super(ResidualConvLayer, self).__init__()\n",
    "        \n",
    "        padding = (kernel_size - 1) // 2\n",
    "        assert (kernel_size - 1) % 2 == 0, 'Provide odd kernel size to use this layer'\n",
    "        \n",
    "        # pre-activations as in Identity Mappings in Deep Residual Networks https://arxiv.org/abs/1603.05027\n",
    "        self.net = nn.Sequential(\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv2d(n_channels, n_channels, kernel_size, stride=1, padding=padding),\n",
    "                                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x) + x\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a maxpool to shrink each linear dimension by a factor of 4 (2 from stride of first conv, 2 for pooling)\n",
    "class StateEncodingConvBlock(nn.Module):\n",
    "    \"\"\" \n",
    "    - First conv layer halves the spatial dimensions\n",
    "    - 2 residual convolutional layers with ReLU pre-activations (they act on the input and not the output)\n",
    "    - 2x2 MaxPool to halve again the dimensions\n",
    "    \"\"\"\n",
    "    def __init__(self, res, in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n",
    "        super(StateEncodingConvBlock, self).__init__()\n",
    "        new_res = (res - kernel_size + 2*padding)//stride + 1\n",
    "        self.new_res = new_res # useful info to access from outside the class\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            ResidualConvLayer(new_res, out_channels, kernel_size=3),\n",
    "            ResidualConvLayer(new_res, out_channels, kernel_size=3),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 16, 16])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = nn.MaxPool2d(2)\n",
    "img = torch.rand(1,1,32,32)\n",
    "out = layer(img)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 32\n",
    "in_channels = screen_channels + 1\n",
    "out_channels = 32\n",
    "conv_block = StateEncodingConvBlock(res, in_channels, out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "screen_tensor = torch.tensor(state_dict['screen_layers']).float().unsqueeze(0)\n",
    "#screen_tensor = torch.rand((1,in_channels, res, res))\n",
    "x_screen = conv_block(screen_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 16, 16])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_screen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell to compute output resolution for a conv layer\n",
    "kernel_size = 4\n",
    "padding = 1\n",
    "stride = 2\n",
    "res = 32\n",
    "new_res = (res - kernel_size + 2*padding)//stride + 1\n",
    "new_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically we will have 2 convolutional blocks, one for the minimap and one for the screen. If we want a shortcut of this we can just merge them together and use a single convolutional block, but of course this way might make a better use of the domain knowledge (i.e. don't treat spatial information at two different scales like if it was from the same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Variable dimensionality recap\n",
    "\n",
    "**screen**: tiling binary mask and adding batch dim: (1, screen_channels, res, res) <br>\n",
    "**minimap**: tiling binary mask and adding batch dim: (1, minimap_channels, res, res) <br>\n",
    "**player**:  adding batch dim: (1, in_player) <br>\n",
    "**last_action**:  adding batch dim: (1,) (good like this because it needs to be embedded afterwards)<br>\n",
    "\n",
    "### After state encoding:\n",
    "\n",
    "**inputs_3D**: (1, #channels_1, new_res, new_res) <br>\n",
    "with `new_res = (res - kernel_size + 2*padding)//stride + 1` and `#channels_1 = out_channels*2` (default 64 and 32 respectively) <br>\n",
    "Uses **StateEncodingConvBlock**.\n",
    "\n",
    "**inputs_2D**: (1, in_player+embed_dim) -> (1,128) -> ReLU -> (1, 64) <br>\n",
    "with default value of embed_dim equal to 10. <br>\n",
    "Uses **Inputs2D_Net**.\n",
    "\n",
    "### After memory processing:\n",
    "Note that inputs_3D are used as input to the Conv2D LSTM and not inputs_2D! Also call the output outputs_3D.\n",
    "\n",
    "Conv2D LSTM: kernel size 3x3, stride 1, (padding of 1 to keep resolution constant), #output_channels 96\n",
    "\n",
    "**outputs_3D**: (1, #output_channels, new_res, new_res) (same spatial resolution) <br>\n",
    "Uses **ConvLSTM**.\n",
    "\n",
    "### Main processing (control part for the relational processing)\n",
    "\n",
    "Input: outputs_3D\n",
    "\n",
    "2 flows:\n",
    "\n",
    "SPATIAL: 12-layer deep residual model ( 4 blocks of 3 convolutional layers each )\n",
    "   - first: kernel 4x4, stride 1 (?)\n",
    "   - second and third: kernel 3x3, stride 1 \n",
    "   - \"interleaved with ReLU activations and skip-connections\" (I don't know if after every layer or every block; <br> ReLUs make sense after each layer, skip connections after every block maybe) \n",
    "    \n",
    "Since the output should have the same shape of the relational-spatial outputs, i.e. [8, 8, #channels2], and inputs_3D already have that spatial resolution in the paper, I would substitute 8 with a more generic new_res and deduce that the whole spatial architecture has padding so that the resolution remains unchanged. #channels2 is not better specified (we can keep it the same as #output_channels for semplicity) <br>\n",
    "Uses **DeepResidualBlock**.\n",
    "\n",
    "NON-SPATIAL: flattened (all three dimensions I guess) and passed to a 2-layer MLP (512 units per layer, ReLU activations) to produce what we refer to above as relational-nonspatial <br>\n",
    "(1, 64 x #channels2) -> (1, 512) -> ReLU -> (1, 512) -> ReLU <br>\n",
    "Uses **NonSpatialBlock**.\n",
    "\n",
    "### Output processing\n",
    "\n",
    "inputs 2D and relational-nonspatial are concatenated to form a set of shared features. <br>\n",
    "Shared features used to produce log probs and V. <br>\n",
    "shared features: \n",
    "- (1, 64 + 512) -> (1, 256) -> ReLU -> (1, #actions) for logits\n",
    "- (1, 64 + 512) -> (1, 256) -> ReLU -> (1, 1) for value\n",
    "\n",
    "Actions are sampled using computed policy logits and embedded into a 16 dimensional vector. \n",
    "\n",
    "This embedding is used to condition shared features and generate logits for non-spatial arguments (Args) through independent linear combinations (one for each argument). [Basically we concatenate the action to shared features and then pass it through a 2-layers MLP?]\n",
    "\n",
    "Finally, spatial arguments (Args x,y ) are obtained by first deconvolving relational-spatial to [32 × 32 × #channels 3 ] tensors using Conv2DTranspose layers, conditioned by tiling the action embedding along the depth dimension and passed to a 1 × 1 × 1 convolution layers (one for each spatial argument). \n",
    "1 x 1 x 1 means a Conv2d with kernel size of 1 and output channels of 1.\n",
    "\n",
    "#channels_3 = 16 <br>\n",
    "Conv2DTranspose: kernel size 4x4, stride 2 (does it work?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv2D_LSTM layer\n",
    "From https://github.com/ndrplz/ConvLSTM_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConvLSTM_pytorch.convlstm import ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = out_channels\n",
    "# in case of a single layer\n",
    "num_layers = 1\n",
    "kernel_size = 3\n",
    "hidden_channels = [96 for _ in range(num_layers)] # not sure about this\n",
    "kernel = [(kernel_size, kernel_size) for _ in range(num_layers)] # pay attention to this\n",
    "conv_lstm = ConvLSTM(input_channels, \n",
    "                     hidden_channels, \n",
    "                     kernel, \n",
    "                     num_layers,\n",
    "                     batch_first=False,\n",
    "                     bias=True,\n",
    "                     return_all_layers=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_test1 = torch.rand(1, 1, input_channels, new_res, new_res)\n",
    "input_test2 = torch.rand(5, 10, input_channels, new_res, new_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input must be 5d (t, b, channels, w, h) or (b, t, channels, w, h) \n",
    "layer_output_list, last_state_list = conv_lstm(input_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 96, 16, 16])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(layer_output_list) == num_layers, \"len(layer_output_list) is %d\"%len(layer_output_list)\n",
    "layer_output_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 96, 16, 16])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(last_state_list) == num_layers, \"len(last_state_list) is %d\"%len(last_state_list)\n",
    "assert len(last_state_list[0]) == 2, \"len(last_state_list[0]) is %d\"%len(last_state_list[0])\n",
    "assert last_state_list[0][0].shape == last_state_list[0][1].shape, 'h and c have different shapes'\n",
    "assert torch.all(last_state_list[0][0] == layer_output_list[0][:,-1,...]), 'they both should be last h of the first layer'\n",
    "last_state_list[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**layer_output_list**: <br>\n",
    "[(b,t,c,w,h), ..., (b,t,c,w,h)] <br>\n",
    "List length equal to number of layers\n",
    "\n",
    "**last_state_list**: <br>\n",
    "[[h,c], ..., [h,c]] <br>\n",
    "with both c and h of shape (b,c,w,h) - no time dimension in hidden and cell states (because is understood that they are relative to the last timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 96, 16, 16])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looping one step at the time and using the previous hidden state as new state of the lstm\n",
    "T = 10\n",
    "hidden_states = None\n",
    "for t in range(T):\n",
    "    layer_output_list, hidden_states = conv_lstm(input_test2, hidden_states)\n",
    "out = layer_output_list[-1]\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = out.sum()\n",
    "R.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step it would make sense (maybe) to merge batch and time dimensions (I'm thinking about the learner).\n",
    "\n",
    "In case of the actors, we just have a batch dimension of 1, so we will add a fake time dimension in front of it (or as second dimension, in a coherent way with the tensors coming out of the buffers that will be used by the learner) and we will shrink it again to 4d afterwards.\n",
    "\n",
    "Just use something like:\n",
    "\n",
    "out = out.view((-1,*out.shape[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 96, 16, 16])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.transpose(1,0).reshape((-1,*out.shape[2:]))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're working with time first and then batch but we receive the output of the lstm as batch first, we need to permute it again before collapsing the two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main processing\n",
    "\n",
    "No idea on how to make a 4x4 convolution without loosing resolution... use 5x5 with padding of 2 in the meanwhile.\n",
    "\n",
    "Also I use a layer skip-connection, since it's not clear if they're using it for every layer or every block.\n",
    "\n",
    "**How can they decide that the output is going to have 32 channels if is a residual block?**\n",
    "\n",
    "At the moment residual layers are lacking BatchNormalization (not possible) and LayerNormalization (possible but not in the original implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConvLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, res, n_channels, kernel_size=3):\n",
    "        super(ResidualConvLayer, self).__init__()\n",
    "        \n",
    "        padding = (kernel_size - 1) // 2\n",
    "        assert (kernel_size - 1) % 2 == 0, 'Provide odd kernel size to use this layer'\n",
    "        \n",
    "        # pre-activations as in Identity Mappings in Deep Residual Networks https://arxiv.org/abs/1603.05027\n",
    "        self.net = nn.Sequential(\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv2d(n_channels, n_channels, kernel_size, stride=1, padding=padding),\n",
    "                                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x) + x\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, res):\n",
    "        super(ResidualConvBlock, self).__init__()\n",
    "        # pre-activations as in Identity Mappings in Deep Residual Networks https://arxiv.org/abs/1603.05027\n",
    "        self.net = nn.Sequential(\n",
    "            ResidualConvLayer(res, in_channels, kernel_size=5),\n",
    "            ResidualConvLayer(res, in_channels, kernel_size=3),\n",
    "            ResidualConvLayer(res, in_channels, kernel_size=3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, res, n_blocks=3):\n",
    "        super(DeepResidualBlock, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            *[ResidualConvBlock(in_channels, res) for _ in range(n_blocks)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we got 96 and 16, we should have 32 and 8 somehow\n",
    "deep_residual_spatial = DeepResidualBlock(in_channels=hidden_channels[-1], res=new_res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_features = deep_residual_spatial(out)\n",
    "spatial_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonSpatialBlock(nn.Module):\n",
    "    def __init__(self, in_channels, res):\n",
    "        super(NonSpatialBlock, self).__init__()\n",
    "        self.flattened_size = in_channels*(res**2)\n",
    "        self.out_features = 512\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.flattened_size)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inputs2D_Net(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_player, \n",
    "        n_actions, \n",
    "        embedding_dim=10\n",
    "    ):\n",
    "        super(Inputs2D_Net, self).__init__()\n",
    "        self.out_features = 64 # in case needed from outside\n",
    "        self.embedding = nn.Embedding(n_actions, embedding_dim, padding_idx=0) # no_op action mapped to 0\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(in_player+embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, player_info, last_action):\n",
    "        \"\"\"\n",
    "        player_info: (batch, in_player)\n",
    "        last_action: (batch,)\n",
    "        \"\"\"\n",
    "        embedded_action = self.embedding(last_action).float()\n",
    "        nonspatial_input = torch.cat([player_info, embedded_action], dim=1)\n",
    "        out = self.MLP(nonspatial_input)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = len(action_names)\n",
    "inputs2d_net = Inputs2D_Net(in_player, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_tensor = torch.tensor(state_dict['player_features']).view(1,-1).float()\n",
    "last_action_tensor = torch.LongTensor([last_action_idx])\n",
    "inputs2d = inputs2d_net(player_tensor, last_action_tensor)\n",
    "inputs2d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorHead(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, \n",
    "        n_shared_features=576\n",
    "    ):\n",
    "        super(ActorHead, self).__init__()\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.actor_net = nn.Sequential(\n",
    "            nn.Linear(n_shared_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, shared_features, mask):\n",
    "        logits = self.actor_net(shared_features)\n",
    "        log_probs = F.log_softmax(logits.masked_fill((mask).bool(), float('-inf')), dim=-1) \n",
    "        return log_probs\n",
    "\n",
    "    \n",
    "class CriticHead(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_shared_features=576\n",
    "    ):\n",
    "        super(CriticHead, self).__init__()\n",
    "        \n",
    "        self.critic_net = nn.Sequential(\n",
    "            nn.Linear(n_shared_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, shared_features):\n",
    "        return self.critic_net(shared_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These were some methods used in the batchedA2C to condition the parameter sampling with the sampled main action. We just need to decide where to define this embedding layer.\n",
    "\n",
    "``` python\n",
    "def _embed_action(self, action):\n",
    "    a = torch.LongTensor(action).to(self.device)\n",
    "    a = self.AC.embedding(a)\n",
    "    return a\n",
    "\n",
    "def _cat_action_to_spatial(self, embedded_action, spatial_repr):\n",
    "    \"\"\" \n",
    "    Assume spatial_repr of shape (B, n_channels, res, res).\n",
    "    Cast embedded_action from (B, embedd_dim) to (B, embedd_dim, res, res)\n",
    "    Concatenate spatial_repr with the broadcasted embedded action along the channel dim.\n",
    "    \"\"\"\n",
    "    res = spatial_repr.shape[-1]\n",
    "    embedded_action = embedded_action.reshape((embedded_action.shape[:2]+(1,1,)))\n",
    "    spatial_a = embedded_action.repeat(1,1,res,res)\n",
    "    spatial_repr = torch.cat([spatial_repr, spatial_a], dim=1)\n",
    "    return spatial_repr\n",
    "\n",
    "def _cat_action_to_nonspatial(self, embedded_action, nonspatial_repr):\n",
    "    \"\"\"\n",
    "    nonspatial_repr: (B, n_features)\n",
    "    embedded_action: (B, embedd_dim)\n",
    "    Concatenate them so that the result is of shape (B, n_features+embedd_dim)\n",
    "    \"\"\"\n",
    "    return torch.cat([nonspatial_repr, embedded_action], dim=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels=96 # last hidden channel of ConvLSTM\n",
    "out_channels=16 # by default\n",
    "kernel_size=4\n",
    "stride=2\n",
    "padding=1\n",
    "conv_transp = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "conv_transp2 = nn.ConvTranspose2d(out_channels, out_channels, kernel_size, stride, padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16, 16, 16])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor = torch.rand(10, in_channels, 8, 8).float()\n",
    "out = conv_transp(test_tensor)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16, 32, 32])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2 = conv_transp2(out)\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally tile the embedded action and use SpatialIMPALA to sample the spatial arguments all in parallel.\n",
    "\n",
    "Actually we don't really need to condition on the sampled action if we use separate networks for each parameter and is simpler to keep at least this part unchanged.\n",
    "\n",
    "Same goes for the categorical parameters...\n",
    "\n",
    "```python\n",
    "class SpatialIMPALA(ParallelSpatialParameters):\n",
    "    def __init__(self, n_channels, linear_size, n_arguments):\n",
    "        super(SpatialIMPALA, self).__init__(n_channels, linear_size, n_arguments)\n",
    "        \n",
    "    def forward(self, x, x_first=True):\n",
    "        B = x.shape[0]\n",
    "        log_probs = self.get_log_probs(x)\n",
    "        probs = torch.exp(log_probs)\n",
    "        index = Categorical(probs).sample() # shape (B, n_args)\n",
    "        # method inherited from ParallelSpatialParameters\n",
    "        y, x = self.unravel_index(index, (self.size,self.size)) # both x and y of shape (B, n_args)\n",
    "        if x_first:\n",
    "            arg_lst = np.array([[xi.detach().numpy(),yi.detach().numpy()] for xi, yi in zip(x,y)])\n",
    "        else:\n",
    "            arg_lst = np.array([[yi.detach().numpy(),xi.detach().numpy()] for xi, yi in zip(x,y)])\n",
    "        arg_lst = arg_lst.transpose(0,2,1)  #shape (batch, n_arguments, [y,x]) (or [x,y])                 \n",
    "        log_prob = log_probs.view(B*self.n_args, self.size**2)[torch.arange(B*self.n_args), index.flatten()]\\\n",
    "                    .view(B, self.n_args) \n",
    "        return arg_lst, log_prob, index\n",
    "    \n",
    "    def get_log_probs(self, x):\n",
    "        \"\"\"Compute flatten log_probs for all arguments - shape: (batch_size, n_args, size**2)\"\"\"\n",
    "        x = self.conv(x)\n",
    "        x = x.reshape((x.shape[0],self.n_args,-1))\n",
    "        log_probs = F.log_softmax(x, dim=(-1))\n",
    "        return log_probs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous flow\n",
    "\n",
    "``` python\n",
    "class IMPALA_AC(ParallelActorCritic)\n",
    "    class ParallelActorCritic(nn.Module)\n",
    "        self.spatial_features_net = spatial_model(**spatial_dict) # custom net defined in main\n",
    "        self.nonspatial_features_net = nonspatial_model(**nonspatial_dict) # custom net defined in main\n",
    "        self.actor = SharedActor(action_space, n_features)\n",
    "        self.critic = SharedCritic(n_features)\n",
    "        # these 2 are overridden by the IMPALA_AC init - I report directly the final version\n",
    "        self.spatial_params_net = SpatialIMPALA(self.n_channels, \n",
    "                                                self.screen_res[0], \n",
    "                                                self.n_spatial_args\n",
    "                                               )\n",
    "        self.categorical_params_net = CategoricalIMPALA(self.n_features, \n",
    "                                                        self.categorical_sizes, \n",
    "                                                        self.n_categorical_args\n",
    "                                                       )\n",
    "    def actor_step(self, env_output)\n",
    "        \"\"\"\n",
    "        Input\n",
    "        -----\n",
    "        env_output: dict with keys [spatial_state, player_state, action_mask]\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        actor_output: dict with keys [log_prob, main_action, sc_env_action, \n",
    "                                        categorical_args_indexes, spatial_args_indexes]\n",
    "        \"\"\"\n",
    "        \n",
    "    def learner_step(self, batch):\n",
    "        \"\"\"\n",
    "        Input\n",
    "        -----\n",
    "        batch: dict contianing tensors of shape (T, B, *other_dims), where \n",
    "                - T = unroll_length (number of steps in the trajectory)\n",
    "                - B = batch_size\n",
    "\n",
    "                Keys: \n",
    "                - spatial_state\n",
    "                - player_state\n",
    "                - spatial_state_trg\n",
    "                - player_state_trg\n",
    "                - action_mask\n",
    "                - main_action\n",
    "                - categorical_indexes\n",
    "                - spatial_indexes\n",
    "                \n",
    "        Return\n",
    "        ------\n",
    "        dict(log_prob=log_prob.view(T,B), \n",
    "             baseline=baseline.view(T,B), \n",
    "             baseline_trg=baseline_trg.view(T,B), \n",
    "             entropy=entropy)\n",
    "        \"\"\"\n",
    "       \n",
    "```\n",
    "\n",
    "After this everything should be the same once again. \n",
    "\n",
    "Changes to do:\n",
    "- new ParallelActorCritic tailored to the new architecture (already all networks inside it)\n",
    "- provide actor step with 'last_action_idx' and 'current_lstm_state'\n",
    "- output 'new_lstm_state' in addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AC_modules.Networks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_params_net = SpatialIMPALA_v2(n_channels=32, linear_size=32, n_arguments=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg_lst:  [[[27 26]\n",
      "  [ 4 27]]]\n",
      "log_prob:  tensor([[-6.9320, -6.9223]], grad_fn=<ViewBackward>)\n",
      "index:  tensor([[859, 868]])\n"
     ]
    }
   ],
   "source": [
    "x_test = torch.rand(1,32,8,8)\n",
    "arg_lst, log_prob, index = spatial_params_net(x_test)\n",
    "print(\"arg_lst: \", arg_lst)\n",
    "print(\"log_prob: \", log_prob)\n",
    "print(\"index: \", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialProcessingBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        res, \n",
    "        screen_channels, \n",
    "        minimap_channels,\n",
    "        encoding_channels,\n",
    "        lstm_channels=96,\n",
    "    ):\n",
    "        assert res%4 == 0, \"Provide an input with resolution divisible by 4\"\n",
    "        self.res = res\n",
    "        self.new_res = res/4\n",
    "        self.lstm_channels = lstm_channels\n",
    "        self.screen_state_enc_net = StateEncodingConvBlock(res, screen_channels, encoding_channels)\n",
    "        self.minimap_state_enc_net = StateEncodingConvBlock(res, minimap_channels, encoding_channels)\n",
    "        self.conv_lsmt = ConvLSTM(\n",
    "                     encoding_channels*2, \n",
    "                     lstm_channels, \n",
    "                     kernel_size=3, \n",
    "                     num_layers=1,\n",
    "                     batch_first=False, # first time dimension, but return is with batch first\n",
    "                     bias=True,\n",
    "                     return_all_layers=True\n",
    "                    )\n",
    "        self.deep_residual_block = DeepResidualBlock(lstm_channels, self.new_res)\n",
    "        self.nonspatial_block = NonSpatialBlock(lstm_channels, self.new_res)\n",
    "        \n",
    "    def forward(self, screen_layers, minimap_layers, hidden_state=None, cell_state=None):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "        ------\n",
    "        screen_layers: (batch_size, screen_channels, res, res)\n",
    "        minimap_layers: (batch_size, minimap_channels, res, res)\n",
    "        hidden_state: (batch_size, lstm_channels, new_res, new_res)\n",
    "        cell_state: (batch_size, lstm_channels, new_res, new_res)\n",
    "        \n",
    "        Intermediate variables\n",
    "        ----------------------\n",
    "        inputs_3D: (batch_size, encoding_channels*2, new_res, new_res)\n",
    "        \"\"\"\n",
    "        # State Encoding\n",
    "        screen_enc = self.screen_state_enc_net(screen_layers)\n",
    "        minimap_enc = self.minimap_state_enc_net(minimap_layers)\n",
    "        inputs_3D = torch.cat([screen_enc, minimap_enc], dim=1) # concatenate along channel dim\n",
    "        \n",
    "        # Memory Processing\n",
    "        if hidden_state is None:\n",
    "            layer_output_list, last_state_list = self.conv_lsmt(inputs_3D)\n",
    "        else:\n",
    "            assert cell_state is not None, \\\n",
    "                \"hidden_state provided, but cell_state is None\"\n",
    "            assert hidden_state.shape == cell_state.shape, \\\n",
    "                (\"hidden_state and cell_state have different shapes\", hidden_state.shape, cell_state.shape)\n",
    "            layer_output_list, last_state_list = self.conv_lsmt(inputs_3D,[(hidden_state, cell_state)])\n",
    "        # output is 5d with batch-first \n",
    "        outputs_3D = layer_output_list[-1].transpose(1,0) # (t,b,c,w,h)\n",
    "        # this works only if num_layers = 1\n",
    "        hidden_state = last_state_list[-1][0]\n",
    "        cell_state = last_state_list[-1][1]\n",
    "        \n",
    "        # Spatial and Non-Spatial Processing\n",
    "        spatial_features = self.deep_residual_block(outputs_3D)\n",
    "        nonspatial_features = self.nonspatial_block(outputs_3D)\n",
    "        \n",
    "        return spatial_features, nonspatial_features, hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All high-level networks with their parameters\n",
    "\n",
    "``` python\n",
    "class SpatialProcessingBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        res, \n",
    "        screen_channels, \n",
    "        minimap_channels,\n",
    "        encoding_channels,\n",
    "        lstm_channels=96,\n",
    "    )\n",
    "    \n",
    "class Inputs2D_Net(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_player, \n",
    "        n_actions, \n",
    "        embedding_dim=10\n",
    "    )\n",
    "        \n",
    "class ActorHead(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, \n",
    "        n_shared_features=576 \n",
    "    )\n",
    "    \n",
    "class CriticHead(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_shared_features=576\n",
    "    )\n",
    "    \n",
    "n_shared_features = SpatialProcessingBlock.NonSpatialBlock.out_features + Inputs2D_Net.out_features\n",
    "\n",
    "class SpatialIMPALA_v2(SpatialIMPALA):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_channels, \n",
    "        linear_size, \n",
    "        n_arguments\n",
    "    ):\n",
    "    \n",
    "n_channels = SpatialProcessingBlock.lstm_channels\n",
    "linear_size = SpatialProcessingBlock.res\n",
    "n_arguments = IMPALA_AC.n_spatial_args (still to be defined)\n",
    "\n",
    "class CategoricalIMPALA(ParallelCategoricalNet):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_features, \n",
    "        sizes, \n",
    "        n_arguments\n",
    "    ):\n",
    "        \n",
    "n_features = n_shared_features\n",
    "sizes = IMPALA_AC.categorical_sizes\n",
    "n_arguments = IMPALA_AC.n_categorical_args\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AC_modules.IMPALA import IMPALA_AC\n",
    "from AC_modules.ActorCriticArchitecture import ParallelActorCritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelActorCritic_v2(ParallelActorCritic, nn.Module):\n",
    "    \"\"\"\n",
    "    Uses as hard-coded architecture the control architecture of paper \n",
    "    Relational Deep Reinforcement Learning [https://arxiv.org/abs/1806.01830]\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        env,\n",
    "        action_names,\n",
    "        screen_channels, \n",
    "        minimap_channels,\n",
    "        encoding_channels,\n",
    "        in_player\n",
    "    ):\n",
    "        # init nn.Module but not ParallelActorCritic \n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "        self.action_names = action_names\n",
    "        self._set_action_table() # creates self.action_table\n",
    "        self.screen_res = env.observation_spec()[0]['feature_screen'][1:]\n",
    "        self.all_actions = env.action_spec()[0][1]\n",
    "        self.all_arguments = env.action_spec()[0][0]\n",
    "        self.action_space = len(action_names)\n",
    "        \n",
    "        # Networks\n",
    "        self.spatial_processing_block = SpatialProcessingBlock(self.screen_res, \n",
    "                                                               screen_channels, \n",
    "                                                               minimap_channels,\n",
    "                                                               encoding_channels\n",
    "                                                              )\n",
    "        self.inputs2d_net = Inputs2D_Net(in_player, self.action_space)\n",
    "        \n",
    "        n_shared_features = self.spatial_processing_block.NonSpatialBlock.out_features + \\\n",
    "                            self.inputs2d_net.out_features\n",
    "        \n",
    "        self.actor = ActorHead(self.action_space, n_shared_features)\n",
    "        self.critic = CriticHead(n_shared_features)\n",
    "        \n",
    "        # take care of computing some useful arguments-related attributes before initializing argument networks\n",
    "        self._init_arg_names()\n",
    "        self._set_spatial_arg_mask()\n",
    "        self._set_categorical_arg_mask()\n",
    "        \n",
    "        self.spatial_params_net = SpatialIMPALA_v2(self.spatial_processing_block.lstm_channels,\n",
    "                                                   self.screen_res[0], \n",
    "                                                   self.n_spatial_args\n",
    "                                                  )\n",
    "        self.categorical_params_net = CategoricalIMPALA(n_shared_features, \n",
    "                                                        self.categorical_sizes, \n",
    "                                                        self.n_categorical_args\n",
    "                                                       )\n",
    "        \n",
    "    def compute_features(\n",
    "        self,\n",
    "        screen_layers, \n",
    "        minimap_layers, \n",
    "        player_info, \n",
    "        last_action, \n",
    "        hidden_state, \n",
    "        cell_state\n",
    "    ):\n",
    "        results = self.spatial_processing_block(screen_layers, minimap_layers, hidden_state, cell_state)\n",
    "        spatial_features, nonspatial_features, hidden_state, cell_state = results\n",
    "        \n",
    "        inputs_2D = self.inputs2d_net(player_info, last_action)\n",
    "        \n",
    "        shared_features = torch.cat([nonspatial_features, inputs_2D], dim=1)\n",
    "        \n",
    "        return spatial_features, shared_features, hidden_state, cell_state\n",
    "    \n",
    "    def pi(self, shared_features, action_mask):\n",
    "        logits = self.actor(shared_features) \n",
    "        log_probs = F.log_softmax(logits.masked_fill((action_mask).bool(), float('-inf')), dim=-1) \n",
    "        return log_probs\n",
    "    \n",
    "    def V_critic(self, shared_features):\n",
    "        return self.critic(shared_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMPALA_AC_v2(ParallelActorCritic_v2, IMPALA_AC):\n",
    "    def __init__(\n",
    "        self, \n",
    "        env,\n",
    "        action_names,\n",
    "        screen_channels, \n",
    "        minimap_channels,\n",
    "        encoding_channels,\n",
    "        in_player,\n",
    "        device\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Notes:\n",
    "        \n",
    "        - ParallelActorCritic_v2 takes the precedence over IMPALA_AC in the inheritance of \n",
    "        methods and attributes in case of conflict (e.g. self.pi and self.V_critic)\n",
    "        \n",
    "        - From IMPALA_AC we keep \n",
    "            sample_spatial_params, \n",
    "            sample_categorical_params, \n",
    "            sample_params,\n",
    "            pad_to_len\n",
    "            \n",
    "        \"\"\"\n",
    "        ParallelActorCritic_v2.__init__(self, \n",
    "                                        env,\n",
    "                                        action_names,\n",
    "                                        screen_channels, \n",
    "                                        minimap_channels,\n",
    "                                        encoding_channels,\n",
    "                                        in_player\n",
    "                                       )\n",
    "        self.device = device \n",
    "        \n",
    "        # number of categorical and spatial arguments that we expect at most for any action \n",
    "        # used for padding arguments and writing them into the buffers always with the same length\n",
    "        \n",
    "        self.max_num_categorical_args = int((self.categorical_arg_mask).sum(axis=1).max()) # should be 1\n",
    "        self.max_num_spatial_args = int((self.spatial_arg_mask).sum(axis=1).max()) # should be 2 because of select_rect\n",
    "        \n",
    "    def actor_step(self, env_output, hidden_state=None, cell_state=None):\n",
    "        screen_layers = env_output['screen_layers'].unsqueeze(0).to(self.device)\n",
    "        minimap_layers = env_output['minimap_layers'].unsqueeze(0).to(self.device)\n",
    "        player_state = env_output['player_state'].unsqueeze(0).to(self.device)\n",
    "        last_action = env_output['last_action'].to(self.device) # add it to the output of the environment\n",
    "        action_mask = env_output['action_mask'].to(self.device)\n",
    "\n",
    "        results = self.compute_features(screen_layers, \n",
    "                                        minimap_layers, \n",
    "                                        player_state, \n",
    "                                        last_action, \n",
    "                                        hidden_state, \n",
    "                                        cell_state\n",
    "                                       )\n",
    "        spatial_features, shared_features, hidden_state, cell_state = results\n",
    "        \n",
    "        log_probs = self.pi(shared_features, action_mask)\n",
    "        probs = torch.exp(log_probs)\n",
    "        main_action_torch = Categorical(probs).sample() # check probs < 0?!\n",
    "        main_action = main_action_torch.detach().cpu().numpy()\n",
    "        log_prob = log_probs[range(len(main_action)), main_action]\n",
    "        \n",
    "        args, args_log_prob, args_indexes = self.sample_params(shared_features, spatial_features, main_action)\n",
    "        assert args_log_prob.shape == log_prob.shape, (\"Shape mismatch between arg_log_prob and log_prob \",\\\n",
    "                                                      args_log_prob.shape, log_prob.shape)\n",
    "        log_prob = log_prob + args_log_prob\n",
    "        \n",
    "        action_id = np.array([self.action_table[act] for act in main_action])\n",
    "        sc2_env_action = [sc_actions.FunctionCall(action_id[i], args[i]) for i in range(len(action_id))]\n",
    "        \n",
    "        actor_output = {'log_prob':log_prob.flatten(),\n",
    "                        'main_action':main_action_torch.flatten(),\n",
    "                        'sc_env_action':sc2_env_action,\n",
    "                        'hidden_state':hidden_state,\n",
    "                        'cell_state':cell_state\n",
    "                        **args_indexes} # args_indexes = {'categorical_args_indexes', 'spatial_args_indexes'}\n",
    "        \n",
    "        return actor_output\n",
    "    \n",
    "    def learner_step(self, batch):\n",
    "        \"\"\"\n",
    "        batch contains tensors of shape (T, B, *other_dims), where \n",
    "        - T = unroll_length (number of steps in the trajectory)\n",
    "        - B = batch_size\n",
    "        \n",
    "        Keywords needed:\n",
    "        - spatial_state\n",
    "        - player_state\n",
    "        - spatial_state_trg\n",
    "        - player_state_trg\n",
    "        - action_mask\n",
    "        - main_action\n",
    "        - categorical_indexes\n",
    "        - spatial_indexes\n",
    "        \"\"\"\n",
    "        screen_layers = batch['screen_layers'].to(self.device)\n",
    "        minimap_layers = batch['minimap_layers'].to(self.device)\n",
    "        player_state = batch['player_state'].to(self.device)\n",
    "        \n",
    "        screen_layers_trg = batch['screen_layers_trg'].to(self.device)\n",
    "        minimap_layers_trg = batch['minimap_layers_trg'].to(self.device)\n",
    "        player_state_trg = batch['player_state_trg'].to(self.device)\n",
    "        last_action = batch['last_action'].to(self.device)\n",
    "        \n",
    "        action_mask = batch['action_mask'].to(self.device)\n",
    "        main_action = batch['main_action'].to(self.device)\n",
    "        categorical_indexes = batch['categorical_indexes'].to(self.device)\n",
    "        spatial_indexes = batch['spatial_indexes'].to(self.device)\n",
    "        hidden_states = batch['hidden_states'].to(self.device)\n",
    "        cell_states = batch['cell_states'].to(self.device)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"screen_layers.shape \", screen_layers.shape)\n",
    "            print(\"minimap_layers.shape \", minimap_layers.shape)\n",
    "            print(\"player_state.shape \", player_state.shape)\n",
    "            print(\"last_action.shape \", last_action.shape)\n",
    "            print(\"action_mask.shape \", action_mask.shape)\n",
    "            print(\"main_action.shape \", main_action.shape)\n",
    "            print(\"categorical_indexes.shape \", categorical_indexes.shape)\n",
    "            print(\"spatial_indexes.shape \", spatial_indexes.shape)\n",
    "            print(\"hidden_states.shape \", hidden_states.shape)\n",
    "            print(\"cell_states.shape \", cell_states.shape)\n",
    "            print(\"self.device \", self.device)\n",
    "            \n",
    "        # useful dimensions\n",
    "        T = spatial_state.shape[0]\n",
    "        B = spatial_state.shape[1]\n",
    "        res = self.screen_res[0]\n",
    "        \n",
    "        # merge all batch and time dimensions - \n",
    "        # I actually need the time dim of the variables entering the conv2d\n",
    "        spatial_state = spatial_state.view((-1,)+spatial_state.shape[2:])\n",
    "        player_state = player_state.view((-1,)+player_state.shape[2:])\n",
    "        spatial_state_trg = spatial_state_trg.view((-1,)+spatial_state_trg.shape[2:])\n",
    "        player_state_trg = player_state_trg.view((-1,)+player_state_trg.shape[2:])\n",
    "        action_mask = action_mask.view((-1,)+action_mask.shape[2:])\n",
    "        main_action = main_action.view((-1,)+main_action.shape[2:])\n",
    "        categorical_indexes = categorical_indexes.view((-1,)+categorical_indexes.shape[2:])\n",
    "        spatial_indexes = spatial_indexes.view((-1,)+spatial_indexes.shape[2:])\n",
    "        \n",
    "        if debug:\n",
    "            print(\"After view: \")\n",
    "            print(\"spatial_state.shape \", spatial_state.shape)\n",
    "            print(\"player_state.shape \", player_state.shape)\n",
    "            print(\"action_mask.shape \", action_mask.shape)\n",
    "            print(\"main_action.shape \", main_action.shape)\n",
    "            print(\"categorical_indexes.shape \", categorical_indexes.shape)\n",
    "            print(\"spatial_indexes.shape \", spatial_indexes.shape)\n",
    "            \n",
    "        #print(\"learner action_mask: \", action_mask)\n",
    "        #print(\"main_action: \", main_action)\n",
    "        #print(\"action_mask[range(len(main_action)), main_action] \", action_mask[range(len(main_action)), main_action])\n",
    "        log_probs, spatial_features, nonspatial_features = self.pi(spatial_state, player_state, action_mask)\n",
    "        #print(\"learner log_probs: \", log_probs)\n",
    "        log_prob = log_probs[range(len(main_action)), main_action]\n",
    "        #print(\"learner log_prob: \", log_prob)\n",
    "        entropy = torch.sum(torch.exp(log_prob) * log_prob) # negative entropy of the main actions\n",
    "        \n",
    "        categorical_log_probs = self.categorical_params_net.get_log_probs(nonspatial_features)\\\n",
    "            .view(B*T, self.n_categorical_args, self.categorical_params_net.max_size)\n",
    "        # self.categorical_arg_mask numpy array -> convert it on the fly to cuda tensor\n",
    "        # main_action cuda tensor\n",
    "        # categorical_mask should be cuda tensor\n",
    "        categorical_arg_mask = torch.tensor(self.categorical_arg_mask).to(self.device)\n",
    "        categorical_mask = categorical_arg_mask[main_action,:].view(-1,self.n_categorical_args)\n",
    "        categorical_indexes = categorical_indexes[categorical_indexes!=-1] # remove padding\n",
    "        batch_index = categorical_mask.nonzero()[:,0]\n",
    "        arg_index = categorical_mask.nonzero()[:,1]\n",
    "        categorical_log_prob = categorical_log_probs[batch_index, arg_index, categorical_indexes]\n",
    "        log_prob = log_prob.index_add(0, batch_index, categorical_log_prob)\n",
    "        \n",
    "        # repeat for spatial params\n",
    "        spatial_log_probs = self.spatial_params_net.get_log_probs(spatial_features)\\\n",
    "            .view(B*T, self.n_spatial_args, res**2)\n",
    "        spatial_arg_mask = torch.tensor(self.spatial_arg_mask).to(self.device)\n",
    "        spatial_mask = spatial_arg_mask[main_action,:].view(-1,self.n_spatial_args)\n",
    "        spatial_indexes = spatial_indexes[spatial_indexes!=-1] # remove padding\n",
    "        batch_index = spatial_mask.nonzero()[:,0]\n",
    "        arg_index = spatial_mask.nonzero()[:,1]\n",
    "        spatial_log_prob = spatial_log_probs[batch_index, arg_index, spatial_indexes]\n",
    "        log_prob = log_prob.index_add(0, batch_index, spatial_log_prob)\n",
    "        \n",
    "        baseline = self.V_critic(nonspatial_features=nonspatial_features)\n",
    "        baseline_trg = self.V_critic(spatial_state=spatial_state_trg, player_state=player_state_trg)\n",
    "        \n",
    "        return dict(log_prob=log_prob.view(T,B), \n",
    "                    baseline=baseline.view(T,B), \n",
    "                    baseline_trg=baseline_trg.view(T,B), \n",
    "                    entropy=entropy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
