{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions as sc_actions\n",
    "from SC_Utils.game_utils import IMPALA_ObsProcesser, FullObsProcesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_game(game_params, map_name='MoveToBeacon', step_multiplier=8, **kwargs):\n",
    "\n",
    "    race = sc2_env.Race(1) # 1 = terran\n",
    "    agent = sc2_env.Agent(race, \"Testv0\") # NamedTuple [race, agent_name]\n",
    "    agent_interface_format = sc2_env.parse_agent_interface_format(**game_params) #AgentInterfaceFormat instance\n",
    "\n",
    "    game_params = dict(map_name=map_name, \n",
    "                       players=[agent], # use a list even for single player\n",
    "                       game_steps_per_episode = 0,\n",
    "                       step_mul = step_multiplier,\n",
    "                       agent_interface_format=[agent_interface_format] # use a list even for single player\n",
    "                       )  \n",
    "    env = sc2_env.SC2Env(**game_params, **kwargs)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Environment parameters\n",
    "RESOLUTION = 32\n",
    "game_params = dict(feature_screen=RESOLUTION, feature_minimap=RESOLUTION, action_space=\"FEATURES\") \n",
    "game_names = ['MoveToBeacon','CollectMineralShards','DefeatRoaches','FindAndDefeatZerglings',\n",
    "              'DefeatZerglingsAndBanelings','CollectMineralsAndGas','BuildMarines']\n",
    "map_name = game_names[1]\n",
    "obs_proc_params = {'select_all':True}\n",
    "op = FullObsProcesser(**obs_proc_params)\n",
    "screen_channels, minimap_channels, in_player = op.get_n_channels()\n",
    "in_channels = screen_channels + minimap_channels \n",
    "\n",
    "\"\"\"\n",
    "# A2C params\n",
    "spatial_model = net.FullyConvPlayerAndSpatial\n",
    "nonspatial_model = net.FullyConvNonSpatial\n",
    "# Internal features, passed inside a dictionary\n",
    "conv_channels = flags.conv_channels #32\n",
    "player_features = flags.player_features #16\n",
    "# Exposed features, passed outside of a dictionary\n",
    "n_channels = conv_channels + player_features #48\n",
    "n_features = flags.n_features #256\n",
    "\n",
    "spatial_dict = {\"in_channels\":in_channels, 'in_player':in_player, \n",
    "                'conv_channels':conv_channels, 'player_features':player_features}\n",
    "nonspatial_dict = {'resolution':RESOLUTION, 'kernel_size':3, 'stride':2, 'n_channels':n_channels}\n",
    "\n",
    "HPs = dict(spatial_model=spatial_model, nonspatial_model=nonspatial_model,\n",
    "       n_features=n_features, n_channels=n_channels, action_names=flags.action_names,\n",
    "       spatial_dict=spatial_dict, nonspatial_dict=nonspatial_dict)\n",
    "game_params['HPs'] = HPs\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = init_game(game_params, map_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action = sc_actions.FunctionCall(actions.FUNCTIONS.no_op.id, [])\n",
    "action = sc_actions.FunctionCall(sc_actions.FUNCTIONS.select_army.id, [[0]]) \n",
    "#action = sc_actions.FunctionCall(actions.FUNCTIONS.Attack_screen.id, [[0],[1,1]])\n",
    "obs = env.step(actions=[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_names = ['no_op', 'move_camera', 'select_point', 'select_rect', 'select_idle_worker', 'select_army', \n",
    "              'Attack_screen','Attack_minimap', 'Build_Barracks_screen', 'Build_CommandCenter_screen',\n",
    "              'Build_Refinery_screen', 'Build_SupplyDepot_screen','Harvest_Gather_SCV_screen', \n",
    "              'Harvest_Return_SCV_quick', 'HoldPosition_quick', 'Move_screen', 'Move_minimap',\n",
    "              'Rally_Workers_screen', 'Rally_Workers_minimap','Train_Marine_quick', 'Train_SCV_quick']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_ids = [sc_actions.FUNCTIONS[a_name].id for a_name in action_names]\n",
    "action_table = np.array([action_ids[i] for i in range(len(action_ids))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMP_op = IMPALA_ObsProcesser(action_table, **obs_proc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Last Action as additional input\n",
    "\n",
    "Obs: All actions that are invalid or equivalent to no-op are not recorded by the environment, so last actions will be [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_action = obs[0].observation['last_actions']\n",
    "print(last_action)\n",
    "if len(last_action) == 0:\n",
    "    last_action = 0\n",
    "else:\n",
    "    last_action = last_action[0]\n",
    "last_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   6   7  12  13  42  44  79  91 268 273 274 331 332 343\n",
      " 344 477 490]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_action_idx = np.where(IMP_op.action_table == last_action)[0][0]\n",
    "print(IMP_op.action_table)\n",
    "IMP_op.action_table[last_action_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need somehow to embed the last action in a meaningful way (I guess that an embedding layer with embedding dim of 10 would do). After that we can simply concatenate player\\_info with last\\_action.\n",
    "\n",
    "The model already has the information about the action space, so we just need to pass the embed\\_dim variable (we can actually keep it constant to 10 for simplicity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Screen / Minimap / Categorical action \n",
    "\n",
    "Task: tile a binary mask to screen and minimap with ones if respectively last action was acting on the screen or on the minimap, with zeros otherwise.\n",
    "\n",
    "How to understand if an action is for screen or minimap? At the moment I just have a spatial vs categorical distinction at the argument level, but nothing screen vs minimap vs other at the main action level.\n",
    "\n",
    "It makes sense to build a look-up table before the beginning of the training to answer this question as fast as possible during runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions = env.action_spec()[0][1]\n",
    "all_arguments = env.action_spec()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_screen(sc_env_action, screen=True):\n",
    "    \"\"\"\n",
    "    Modify this function in a method for some class that has access to the action specs\n",
    "    (could be the wrapped Environment class, with self.env instead of env)\n",
    "    \"\"\"\n",
    "    all_actions = env.action_spec()[0][1]\n",
    "    all_arguments = env.action_spec()[0][0]\n",
    "    \n",
    "    ###\n",
    "    args = all_actions[sc_env_action].args\n",
    "    names = [all_arguments[arg.id].name for arg in args]\n",
    "    if screen:\n",
    "        return np.any(['screen' in n for n in names])\n",
    "    else:\n",
    "        return np.any(['minimap' in n for n in names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_if_screen(last_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screen_mask = list(map(check_if_screen, IMP_op.action_table))\n",
    "screen_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimap_mask = list(map(lambda x: check_if_screen(x, False), IMP_op.action_table))\n",
    "minimap_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict, names = IMP_op.get_state(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['screen_layers', 'minimap_layers', 'player_features'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 32, 32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['screen_layers'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 32, 32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['minimap_layers'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['player_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 32, 32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple access during run time\n",
    "screen_binary_mask = np.array([screen_mask[last_action_idx]])\n",
    "screen_binary_mask2D = np.tile(screen_binary_mask, [1,32,32])\n",
    "state_dict['screen_layers'] = np.concatenate([state_dict['screen_layers'], screen_binary_mask2D])\n",
    "state_dict['screen_layers'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course same thing for minimap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Spatial input processing\n",
    "\n",
    "*Spatially encoded inputs (minimap and screen) are tiled with binary masks denoting\n",
    "whether the previous action constituted a screen- or minimap-related action. These tensors are then fed to\n",
    "independent residual convolutional blocks, each consisting of one convolutional layer (4 × 4 kernels and stride\n",
    "2) followed by a residual block with 2 convolutional layers (3 × 3 kernels and stride 1), which process and\n",
    "downsample the inputs to [8 × 8 × #channels 1 ] outputs. These tensors are concatenated along the depth\n",
    "dimension to form a singular spatial input (inputs 3D ).*\n",
    "\n",
    "Differences with previous implementation: \n",
    "1. First process them, then merge them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConvolutional(nn.Module):\n",
    "    \n",
    "    def __init__(self, res, n_channels, hidden_channels=12, kernel_size=3):\n",
    "        super(ResidualConvolutional, self).__init__()\n",
    "        \n",
    "        padding = (kernel_size - 1) // 2\n",
    "        assert (kernel_size - 1) % 2 == 0, 'Provide odd kernel size to use this layer'\n",
    "        \n",
    "        # pre-activations as in Identity Mappings in Deep Residual Networks https://arxiv.org/abs/1603.05027\n",
    "        self.net = nn.Sequential(\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv2d(n_channels, hidden_channels, kernel_size, stride=1, padding=padding),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv2d(hidden_channels, n_channels, kernel_size, stride=1, padding=padding)\n",
    "                                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x) + x\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\" At the moment without residual stuff, just for dimensionality check \"\"\"\n",
    "    def __init__(self, res, in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        new_res = (res - kernel_size + 2*padding)//stride + 1\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            ResidualConvolutional(new_res, out_channels, kernel_size=3),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 32\n",
    "in_channels = screen_channels + 1\n",
    "out_channels = 32\n",
    "conv_block = ConvBlock(res, in_channels, out_channels, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "screen_tensor = torch.tensor(state_dict['screen_layers']).float().unsqueeze(0)\n",
    "#screen_tensor = torch.rand((1,in_channels, res, res))\n",
    "x_screen = conv_block(screen_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 31, 31])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_screen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size = 5\n",
    "padding = 2\n",
    "stride = 1\n",
    "res = 32\n",
    "new_res = (res - kernel_size + 2*padding)//stride + 1\n",
    "new_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically we will have 2 convolutional blocks, one for the minimap and one for the screen. If we want a shortcut of this we can just merge them together and use a single convolutional block, but of course this way might make a better use of the domani knowledge (i.e. don't treat spatial information at two different scales like if it was from the same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Variable dimensionality recap\n",
    "\n",
    "**screen**: tiling binary mask and adding batch dim: (1, screen_channels, res, res) <br>\n",
    "**minimap**: tiling binary mask and adding batch dim: (1, minimap_channels, res, res) <br>\n",
    "**player**:  adding batch dim: (1, in_player) <br>\n",
    "**last_action**:  adding batch dim: (1, 1) <br>\n",
    "\n",
    "### After state encoding:\n",
    "\n",
    "**inputs_3D**: (1, #channels_1, new_res, new_res) <br>\n",
    "with `new_res = (res - kernel_size + 2*padding)//stride + 1` and `#channels_1 = out_channels*2` (default 64 and 32 respectively)\n",
    "\n",
    "**inputs_2D**: (1, in_player+embed_dim) -> (1,128) -> ReLU -> (1, 64) <br>\n",
    "with default value of embed_dim equal to 10.\n",
    "\n",
    "### After memory processing:\n",
    "Note that inputs_3D are used as input to the Conv2D LSTM and not inputs_2D! Also call the output outputs_3D.\n",
    "\n",
    "Conv2D LSTM: kernel size 3x3, stride 1, (padding of 1 to keep resolution constant), #output_channels 96\n",
    "\n",
    "**outputs_3D**: (1, #output_channels, new_res, new_res) (same spatial resolution)\n",
    "\n",
    "### Main processing (control part for the relational processing)\n",
    "\n",
    "Input: outputs_3D\n",
    "\n",
    "2 flows:\n",
    "\n",
    "SPATIAL: 12-layer deep residual model ( 4 blocks of 3 convolutional layers each )\n",
    "   - first: kernel 4x4, stride 1 (?)\n",
    "   - second and third: kernel 3x3, stride 1 \n",
    "   - \"interleaved with ReLU activations and skip-connections\" (I don't know if after every layer or every block; <br> ReLUs make sense after each layer, skip connections after every block maybe) \n",
    "    \n",
    "Since the output should have the same shape of the relational-spatial outputs, i.e. [8, 8, #channels2], and inputs_3D already have that spatial resolution in the paper, I would substitute 8 with a more generic new_res and deduce that the whole spatial architecture has padding so that the resolution remains unchanged. #channels2 is not better specified (we can keep it the same as #output_channels for semplicity)\n",
    "\n",
    "NON-SPATIAL: flattened (all three dimensions I guess) and passed to a 2-layer MLP (512 units per layer, ReLU activations) to produce what we refer to above as relational-nonspatial <br>\n",
    "(1, 64 x #channels2) -> (1, 512) -> ReLU -> (1, 512) -> ReLU\n",
    "\n",
    "### Output processing\n",
    "\n",
    "inputs 2D and relational-nonspatial are concatenated to form a set of shared features. <br>\n",
    "Shared features used to produce log probs and V. <br>\n",
    "shared features: \n",
    "- (1, 64 + 512) -> (1, 256) -> ReLU -> (1, #actions) for logits\n",
    "- (1, 64 + 512) -> (1, 256) -> ReLU -> (1, 1) for value\n",
    "\n",
    "Actions are sampled using computed policy logits and embedded into a 16 dimensional vector. \n",
    "\n",
    "This embedding is used to condition shared features and generate logits for non-spatial arguments (Args) through independent linear combinations (one for each argument). [Basically we concatenate the action to shared features and then pass it through a 2-layers MLP?]\n",
    "\n",
    "Finally, spatial arguments (Args x,y ) are obtained by first deconvolving relational-spatial to [32 × 32 × #channels 3 ] tensors using Conv2DTranspose layers, conditioned by tiling the action embedding along the depth dimension and passed to a 1 × 1 × 1 convolution layers (one for each spatial argument). \n",
    "1 x 1 x 1 means a Conv2d with kernel size of 1 and output channels of 1.\n",
    "\n",
    "#channels_3 = 16 <br>\n",
    "Conv2DTranspose: kernel size 4x4, stride 2 (does it work?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv2D_LSTM layer\n",
    "From https://github.com/ndrplz/ConvLSTM_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConvLSTM_pytorch.convlstm import ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = out_channels\n",
    "# in case of a single layer\n",
    "num_layers = 1\n",
    "kernel_size = 3\n",
    "hidden_channels = [96 for _ in range(num_layers)] # not sure about this\n",
    "kernel = [(kernel_size, kernel_size) for _ in range(num_layers)] # pay attention to this\n",
    "conv_lstm = ConvLSTM(input_channels, \n",
    "                     hidden_channels, \n",
    "                     kernel, \n",
    "                     num_layers,\n",
    "                     batch_first=False,\n",
    "                     bias=True,\n",
    "                     return_all_layers=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_test1 = torch.rand(1, 1, input_channels, new_res, new_res)\n",
    "input_test2 = torch.rand(5, 10, input_channels, new_res, new_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input must be 5d (t, b, channels, w, h) or (b, t, channels, w, h) \n",
    "layer_output_list, last_state_list = conv_lstm(input_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 96, 16, 16])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(layer_output_list) == num_layers, \"len(layer_output_list) is %d\"%len(layer_output_list)\n",
    "layer_output_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 96, 16, 16])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(last_state_list) == num_layers, \"len(last_state_list) is %d\"%len(last_state_list)\n",
    "assert len(last_state_list[0]) == 2, \"len(last_state_list[0]) is %d\"%len(last_state_list[0])\n",
    "assert last_state_list[0][0].shape == last_state_list[0][1].shape, 'h and c have different shapes'\n",
    "assert torch.all(last_state_list[0][0] == layer_output_list[0][:,-1,...]), 'they both should be last h of the first layer'\n",
    "last_state_list[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**layer_output_list**: <br>\n",
    "[(b,t,c,w,h), ..., (b,t,c,w,h)] <br>\n",
    "List length equal to number of layers\n",
    "\n",
    "**last_state_list**: <br>\n",
    "[[h,c], ..., [h,c]] <br>\n",
    "with both c and h of shape (b,c,w,h) - no time dimension in hidden and cell states (because is understood that they are relative to the last timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 96, 16, 16])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looping one step at the time and using the previous hidden state as new state of the lstm\n",
    "T = 10\n",
    "hidden_states = None\n",
    "for t in range(T):\n",
    "    layer_output_list, hidden_states = conv_lstm(input_test2, hidden_states)\n",
    "out = layer_output_list[-1]\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step it would make sense (maybe) to merge batch and time dimensions (I'm thinking about the learner).\n",
    "\n",
    "In case of the actors, we just have a batch dimension of 1, so we will add a fake time dimension in front of it (or as second dimension, in a coherent way with the tensors coming out of the buffers that will be used by the learner) and we will shrink it again to 4d afterwards.\n",
    "\n",
    "Just use something like:\n",
    "\n",
    "out = out.view((-1,*out.shape[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 96, 16, 16])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.transpose(1,0).reshape((-1,*out.shape[2:]))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're working with time first and then batch but we receive the output of the lstm as batch first, we need to permute it again before collapsing the two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main processing\n",
    "\n",
    "No idea on how to make a 4x4 convolution without loosing resolution... use 5x5 with padding of 2 in the meanwhile.\n",
    "\n",
    "Also I use a layer skip-connection, since it's not clear if they're using it for every layer or every block.\n",
    "\n",
    "**How can they decide that the output is going to have 32 channels if is a residual block?**\n",
    "\n",
    "At the moment residual layers are lacking BatchNormalization (not possible) and LayerNormalization (possible but not in the original implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConvLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, res, n_channels, kernel_size=3):\n",
    "        super(ResidualConvLayer, self).__init__()\n",
    "        \n",
    "        padding = (kernel_size - 1) // 2\n",
    "        assert (kernel_size - 1) % 2 == 0, 'Provide odd kernel size to use this layer'\n",
    "        \n",
    "        # pre-activations as in Identity Mappings in Deep Residual Networks https://arxiv.org/abs/1603.05027\n",
    "        self.net = nn.Sequential(\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv2d(n_channels, n_channels, kernel_size, stride=1, padding=padding),\n",
    "                                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x) + x\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, res):\n",
    "        super(ResidualConvBlock, self).__init__()\n",
    "        # pre-activations as in Identity Mappings in Deep Residual Networks https://arxiv.org/abs/1603.05027\n",
    "        self.net = nn.Sequential(\n",
    "            ResidualConvLayer(res, in_channels, kernel_size=5),\n",
    "            ResidualConvLayer(res, in_channels, kernel_size=3),\n",
    "            ResidualConvLayer(res, in_channels, kernel_size=3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, res, n_blocks=3):\n",
    "        super(DeepResidualBlock, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            *[ResidualConvBlock(in_channels, res) for _ in range(n_blocks)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we got 96 and 16, we should have 32 and 8 somehow\n",
    "deep_residual_spatial = DeepResidualBlock(in_channels=hidden_channels[-1], res=new_res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_features = deep_residual_spatial(out)\n",
    "spatial_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonSpatialBlock(nn.Module):\n",
    "    def __init__(self, in_channels, res):\n",
    "        super(NonSpatialBlock, self).__init__()\n",
    "        self.flattened_size = in_channels*(res**2)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.flattened_size)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inputs2D_Net(nn.Module):\n",
    "    def __init__(self, in_player, n_actions, embedding_dim=10):\n",
    "        super(Inputs2D_Net, self).__init__()\n",
    "        self.out_features = 64 # in case needed from outside\n",
    "        self.embedding = nn.Embedding(n_actions, embedding_dim, padding_idx=0) # no_op action mapped to 0\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(in_player+embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, player_info, last_action):\n",
    "        \"\"\"\n",
    "        player_info: (batch, in_player)\n",
    "        last_action: (batch,)\n",
    "        \"\"\"\n",
    "        embedded_action = self.embedding(last_action).float()\n",
    "        nonspatial_input = torch.cat([player_info, embedded_action], dim=1)\n",
    "        out = self.MLP(nonspatial_input)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = len(action_names)\n",
    "inputs2d_net = Inputs2D_Net(in_player, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_tensor = torch.tensor(state_dict['player_features']).view(1,-1).float()\n",
    "last_action_tensor = torch.LongTensor([last_action_idx])\n",
    "inputs2d = inputs2d_net(player_tensor, last_action_tensor)\n",
    "inputs2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticHead(nn.Module):\n",
    "    def __init__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
