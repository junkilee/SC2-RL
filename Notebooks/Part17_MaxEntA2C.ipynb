{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "from SC_Utils.game_utils import ObsProcesser, get_action_dict\n",
    "from SC_Utils.train_MaxEnt import *\n",
    "from AC_modules.BatchedA2C import SpatialA2C_MaxEnt_v2\n",
    "import AC_modules.Networks as net\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "RESOLUTION = 32\n",
    "game_params = dict(feature_screen=RESOLUTION, feature_minimap=RESOLUTION, action_space=\"FEATURES\") \n",
    "game_names = {1:'MoveToBeacon',\n",
    "              2:'CollectMineralShards',\n",
    "              3:'DefeatRoaches',\n",
    "              4:'FindAndDefeatZerglings',\n",
    "              5:'DefeatZerglingsAndBanelings',\n",
    "              6:'CollectMineralsAndGas',\n",
    "              7:'BuildMarines'\n",
    "              }\n",
    "map_name = game_names[1]\n",
    "\n",
    "# Observation Processer parameters\n",
    "#screen_names = ['visibility_map', 'player_relative', 'selected', 'unit_density', 'unit_density_aa']\n",
    "#minimap_names = []\n",
    "#obs_proc_params = {'screen_names':screen_names, 'minimap_names':minimap_names}\n",
    "obs_proc_params = {'select_all':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = init_game(game_params, map_name)\n",
    "op = ObsProcesser(**obs_proc_params)\n",
    "screen_channels, minimap_channels = op.get_n_channels()\n",
    "in_channels = screen_channels + minimap_channels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_names = ['select_army', 'Move_screen','Attack_screen']\n",
    "#action_names = ['select_army','Move_screen']\n",
    "#action_names = ['select_army', 'Attack_screen', 'Move_screen', 'select_point', 'select_rect',\n",
    "#                'move_camera','Stop_quick','Move_minimap','Attack_minimap','HoldPosition_quick']\n",
    "action_dict = get_action_dict(action_names)\n",
    "action_space = len(action_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_model = net.FullyConvSpatial\n",
    "nonspatial_model = net.FullyConvNonSpatial\n",
    "n_channels = 32\n",
    "n_features = 256\n",
    "spatial_dict = {\"in_channels\":in_channels}\n",
    "nonspatial_dict = {'resolution':RESOLUTION, 'kernel_size':3, 'stride':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "HPs = dict(action_space=action_space, gamma=0.99, n_steps=20, H=7e-3, \n",
    "           spatial_model=spatial_model, nonspatial_model=nonspatial_model,\n",
    "           n_features=n_features, n_channels=n_channels, \n",
    "           spatial_dict=spatial_dict, nonspatial_dict=nonspatial_dict, \n",
    "           action_dict=action_dict)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    HPs['device'] = 'cuda'\n",
    "else:\n",
    "    HPs['device'] = 'cpu'\n",
    "    \n",
    "print(\"Using device \"+HPs['device'])\n",
    "\n",
    "lr = 7e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SpatialA2C_MaxEnt_v2(env=env, **HPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unroll_length = 60\n",
    "\n",
    "train_dict = dict(n_train_processes = 11,\n",
    "                  max_train_steps = unroll_length*1000,\n",
    "                  unroll_length = unroll_length,\n",
    "                  test_interval = unroll_length*10,\n",
    "                  inspection_interval = unroll_length*10\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <_Functions.select_army: 7>,\n",
       " 1: <_Functions.Move_screen: 331>,\n",
       " 2: <_Functions.Attack_screen: 12>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpatialActorCritic_v2(\n",
       "  (spatial_features_net): FullyConvSpatial(\n",
       "    (net): Sequential(\n",
       "      (0): Conv2d(38, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (nonspatial_features_net): FullyConvNonSpatial(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=14400, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (actor): SharedActor(\n",
       "    (linear): Linear(in_features=256, out_features=3, bias=True)\n",
       "  )\n",
       "  (critic): SharedCritic(\n",
       "    (net): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (arguments_networks): ModuleDict(\n",
       "    (Attack_screen/queued): CategoricalNet(\n",
       "      (net): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=256, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (Attack_screen/screen): SpatialParameters(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (Move_screen/queued): CategoricalNet(\n",
       "      (net): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=256, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (Move_screen/screen): SpatialParameters(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (select_army/select_add): CategoricalNet(\n",
       "      (net): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=256, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['select_army/select_add'],\n",
       " 1: ['Move_screen/queued', 'Move_screen/screen'],\n",
       " 2: ['Attack_screen/queued', 'Attack_screen/screen']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.AC.act_to_arg_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'select_army/select_add': 7,\n",
       " 'Move_screen/queued': 3,\n",
       " 'Move_screen/screen': 0,\n",
       " 'Attack_screen/queued': 3,\n",
       " 'Attack_screen/screen': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.AC.arguments_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ID:  PXFS\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = train_batched_A2C(agent, game_params, map_name, lr, \n",
    "                            obs_proc_params=obs_proc_params, action_dict=action_dict, **train_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array([[0,0,1,0,1],\n",
    "                    [0,0,0,0,1]])\n",
    "\n",
    "done = np.array([[0,0,1,0,1],\n",
    "                 [0,0,0,0,1]])\n",
    "\n",
    "B = rewards.shape[0]\n",
    "T = rewards.shape[1]\n",
    "n_steps = 3\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce simil-entropies with gradient associated\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Linear(1,4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.net(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = torch.rand(B,T,1)\n",
    "actor = Actor()\n",
    "log_probs = actor(states)\n",
    "probs = torch.exp(log_probs) \n",
    "distr = Categorical(probs=probs)\n",
    "entropy = distr.entropy() # POSITIVE\n",
    "entropy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3863, -1.3863, -1.3863, -1.3863, -1.3863],\n",
       "        [-1.3863, -1.3863, -1.3863, -1.3863, -1.3863]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_entropy = torch.log(torch.ones(probs.shape)/probs.shape[2]).mean(axis=-1) # negative\n",
    "rel_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we basically have to add between them the entropy (multiplied by a factor H in general) and the rewards, and then perform what the function compute_n_step_rewards does, but in pytorch, so that the computational graph will be preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "def compute_n_step_rewards(self, rewards, done, n_steps=None):\n",
    "        \"\"\"\n",
    "        Computes n-steps discounted reward. \n",
    "        Note: the rewards considered are AT MOST n, but can be less for the last n-1 elements.\n",
    "        \"\"\"\n",
    "        if n_steps is None:\n",
    "            n_steps = self.n_steps\n",
    "        B = done.shape[0]\n",
    "        T = done.shape[1]\n",
    "        \n",
    "        # Compute episode mask (i-th row contains 1 if col j is in the same episode of col i, 0 otherwise)\n",
    "        episode_mask = [[] for _ in range(B)]\n",
    "        last = [-1 for _ in range(B)]\n",
    "        xs, ys = np.nonzero(done)\n",
    "        \n",
    "        # Add done at the end of every batch to avoid exceptions -> not used in real target computations\n",
    "        xs = np.concatenate([xs, np.arange(B)])\n",
    "        ys = np.concatenate([ys, np.full(B, T-1)])\n",
    "        for x, y in zip(xs, ys):\n",
    "            m = [1 if (i > last[x] and i <= y) else 0 for i in range(T)]\n",
    "            for _ in range(y-last[x]):\n",
    "                episode_mask[x].append(m)\n",
    "            last[x] = y\n",
    "        episode_mask = np.array(episode_mask)\n",
    "        \n",
    "        # Compute n-steps mask and repeat it B times\n",
    "        n_steps_mask = []\n",
    "        for i in range(T):\n",
    "            m = [1 if (j>=i and j<i+n_steps) else 0 for j in range(T)]\n",
    "            n_steps_mask.append(m)\n",
    "        n_steps_mask = np.array(n_steps_mask)\n",
    "        n_steps_mask_b = np.repeat(n_steps_mask[np.newaxis,...] , B, axis=0)\n",
    "        \n",
    "        # Broadcast rewards to use multiplicative masks\n",
    "        rewards_repeated = np.repeat(rewards[:,np.newaxis,:], T, axis=1)\n",
    "        \n",
    "        # Exponential discount factor\n",
    "        Gamma = np.array([self.gamma**i for i in range(T)]).reshape(1,-1)\n",
    "        n_steps_r = (Gamma*rewards_repeated*episode_mask*n_steps_mask_b).sum(axis=2)/Gamma\n",
    "        return n_steps_r, episode_mask, n_steps_mask_b\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_mask.shape:  (2, 5, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute episode mask (i-th row contains 1 if col j is in the same episode of col i, 0 otherwise)\n",
    "episode_mask = [[] for _ in range(B)]\n",
    "last = [-1 for _ in range(B)]\n",
    "xs, ys = np.nonzero(done)\n",
    "\n",
    "# Add done at the end of every batch to avoid exceptions -> not used in real target computations\n",
    "xs = np.concatenate([xs, np.arange(B)])\n",
    "ys = np.concatenate([ys, np.full(B, T-1)])\n",
    "for x, y in zip(xs, ys):\n",
    "    m = [1 if (i > last[x] and i <= y) else 0 for i in range(T)]\n",
    "    for _ in range(y-last[x]):\n",
    "        episode_mask[x].append(m)\n",
    "    last[x] = y\n",
    "episode_mask = np.array(episode_mask)\n",
    "print(\"episode_mask.shape: \", episode_mask.shape)\n",
    "episode_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps_mask_b.shape:  (2, 5, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1, 1, 1, 0, 0],\n",
       "        [0, 1, 1, 1, 0],\n",
       "        [0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 1]],\n",
       "\n",
       "       [[1, 1, 1, 0, 0],\n",
       "        [0, 1, 1, 1, 0],\n",
       "        [0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 1]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute n-steps mask and repeat it B times\n",
    "n_steps_mask = []\n",
    "for i in range(T):\n",
    "    m = [1 if (j>=i and j<i+n_steps) else 0 for j in range(T)]\n",
    "    n_steps_mask.append(m)\n",
    "n_steps_mask = np.array(n_steps_mask)\n",
    "n_steps_mask_b = np.repeat(n_steps_mask[np.newaxis,...] , B, axis=0)\n",
    "print(\"n_steps_mask_b.shape: \", n_steps_mask_b.shape)\n",
    "n_steps_mask_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.3613, 1.3393, 2.3468, 1.2158, 2.3600],\n",
       "         [1.3613, 1.3393, 2.3468, 1.2158, 2.3600],\n",
       "         [1.3613, 1.3393, 2.3468, 1.2158, 2.3600],\n",
       "         [1.3613, 1.3393, 2.3468, 1.2158, 2.3600],\n",
       "         [1.3613, 1.3393, 2.3468, 1.2158, 2.3600]],\n",
       "\n",
       "        [[1.2363, 1.3442, 1.2788, 1.3432, 2.3325],\n",
       "         [1.2363, 1.3442, 1.2788, 1.3432, 2.3325],\n",
       "         [1.2363, 1.3442, 1.2788, 1.3432, 2.3325],\n",
       "         [1.2363, 1.3442, 1.2788, 1.3432, 2.3325],\n",
       "         [1.2363, 1.3442, 1.2788, 1.3432, 2.3325]]], grad_fn=<RepeatBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = torch.tensor(rewards, dtype=torch.float32)\n",
    "augmented_r = r + entropy\n",
    "rewards_repeated = augmented_r.view(B,1,T).repeat(1,T,1)\n",
    "rewards_repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards_repeated.shape:  (2, 5, 5)\n",
      "rewards_repeated:  [[[0 0 1 0 1]\n",
      "  [0 0 1 0 1]\n",
      "  [0 0 1 0 1]\n",
      "  [0 0 1 0 1]\n",
      "  [0 0 1 0 1]]\n",
      "\n",
      " [[0 0 0 0 1]\n",
      "  [0 0 0 0 1]\n",
      "  [0 0 0 0 1]\n",
      "  [0 0 0 0 1]\n",
      "  [0 0 0 0 1]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1.3613, 1.3393, 2.3468, 1.2158, 2.3600],\n",
       "         [1.3613, 1.3393, 2.3468, 1.2158, 2.3600],\n",
       "         [1.3613, 1.3393, 2.3468, 1.2158, 2.3600],\n",
       "         [1.3613, 1.3393, 2.3468, 1.2158, 2.3600],\n",
       "         [1.3613, 1.3393, 2.3468, 1.2158, 2.3600]],\n",
       "\n",
       "        [[1.2363, 1.3442, 1.2788, 1.3432, 2.3325],\n",
       "         [1.2363, 1.3442, 1.2788, 1.3432, 2.3325],\n",
       "         [1.2363, 1.3442, 1.2788, 1.3432, 2.3325],\n",
       "         [1.2363, 1.3442, 1.2788, 1.3432, 2.3325],\n",
       "         [1.2363, 1.3442, 1.2788, 1.3432, 2.3325]]], dtype=torch.float64,\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Broadcast rewards to use multiplicative masks\n",
    "rewards_repeated = np.repeat(rewards[:,np.newaxis,:], T, axis=1)\n",
    "print(\"rewards_repeated.shape: \", rewards_repeated.shape)\n",
    "print(\"rewards_repeated: \", rewards_repeated)\n",
    "rewards_repeated = torch.tensor(rewards_repeated, dtype=float)\n",
    "augmented_r = rewards_repeated + entropy.view(B,1,T) # broadcasting along correct axis\n",
    "augmented_r.shape\n",
    "augmented_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential discount factor\n",
    "Gamma = torch.tensor([gamma**i for i in range(T)]).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_episode_mask = torch.tensor(episode_mask)\n",
    "t_n_steps_mask_b = torch.tensor(n_steps_mask_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.9874, 3.6627, 2.3468, 3.5521, 2.3600],\n",
       "        [3.8204, 3.9266, 4.8946, 3.6523, 2.3325]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps_r = torch.sum(Gamma*rewards_repeated*t_episode_mask*t_n_steps_mask_b, axis=2)/Gamma\n",
    "n_steps_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_step_rewards(rewards, done, entropies, n_steps=3, gamma=0.99):\n",
    "        \"\"\"\n",
    "        Computes n-steps discounted reward. \n",
    "        Note: the rewards considered are AT MOST n, but can be less for the last n-1 elements.\n",
    "        \"\"\"\n",
    "        B = done.shape[0]\n",
    "        T = done.shape[1]\n",
    "\n",
    "        # Compute episode mask (i-th row contains 1 if col j is in the same episode of col i, 0 otherwise)\n",
    "        episode_mask = [[] for _ in range(B)]\n",
    "        last = [-1 for _ in range(B)]\n",
    "        xs, ys = np.nonzero(done)\n",
    "\n",
    "        # Add done at the end of every batch to avoid exceptions -> not used in real target computations\n",
    "        xs = np.concatenate([xs, np.arange(B)])\n",
    "        ys = np.concatenate([ys, np.full(B, T-1)])\n",
    "        for x, y in zip(xs, ys):\n",
    "            m = [1 if (i > last[x] and i <= y) else 0 for i in range(T)]\n",
    "            for _ in range(y-last[x]):\n",
    "                episode_mask[x].append(m)\n",
    "            last[x] = y\n",
    "        episode_mask = np.array(episode_mask)\n",
    "\n",
    "        # Compute n-steps mask and repeat it B times\n",
    "        n_steps_mask = []\n",
    "        for i in range(T):\n",
    "            m = [1 if (j>=i and j<i+n_steps) else 0 for j in range(T)]\n",
    "            n_steps_mask.append(m)\n",
    "        n_steps_mask = np.array(n_steps_mask)\n",
    "        n_steps_mask_b = np.repeat(n_steps_mask[np.newaxis,...] , B, axis=0)\n",
    "\n",
    "        r = torch.tensor(rewards, dtype=torch.float64)\n",
    "        augmented_r = r + entropy\n",
    "        rewards_repeated = augmented_r.view(B,1,T).repeat(1,T,1)\n",
    "\n",
    "        # Exponential discount factor\n",
    "        Gamma = torch.tensor([gamma**i for i in range(T)]).reshape(1,-1)\n",
    "        t_episode_mask = torch.tensor(episode_mask)\n",
    "        t_n_steps_mask_b = torch.tensor(n_steps_mask_b)\n",
    "        n_steps_r = torch.sum(Gamma*rewards_repeated*t_episode_mask*t_n_steps_mask_b, axis=2)/Gamma\n",
    "        \n",
    "        return n_steps_r, episode_mask, n_steps_mask_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_n_steps_r, t_episode_mask, t_n_steps_mask_b = compute_n_step_rewards(rewards, done, entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_check(rewards, done, n_steps=3, gamma=0.99):\n",
    "    B = done.shape[0]\n",
    "    T = done.shape[1]\n",
    "\n",
    "    # Compute episode mask (i-th row contains 1 if col j is in the same episode of col i, 0 otherwise)\n",
    "    episode_mask = [[] for _ in range(B)]\n",
    "    last = [-1 for _ in range(B)]\n",
    "    xs, ys = np.nonzero(done)\n",
    "\n",
    "    # Add done at the end of every batch to avoid exceptions -> not used in real target computations\n",
    "    xs = np.concatenate([xs, np.arange(B)])\n",
    "    ys = np.concatenate([ys, np.full(B, T-1)])\n",
    "    for x, y in zip(xs, ys):\n",
    "        m = [1 if (i > last[x] and i <= y) else 0 for i in range(T)]\n",
    "        for _ in range(y-last[x]):\n",
    "            episode_mask[x].append(m)\n",
    "        last[x] = y\n",
    "    episode_mask = np.array(episode_mask)\n",
    "\n",
    "    # Compute n-steps mask and repeat it B times\n",
    "    n_steps_mask = []\n",
    "    for i in range(T):\n",
    "        m = [1 if (j>=i and j<i+n_steps) else 0 for j in range(T)]\n",
    "        n_steps_mask.append(m)\n",
    "    n_steps_mask = np.array(n_steps_mask)\n",
    "    n_steps_mask_b = np.repeat(n_steps_mask[np.newaxis,...] , B, axis=0)\n",
    "\n",
    "    # Broadcast rewards to use multiplicative masks\n",
    "    rewards_repeated = np.repeat(rewards[:,np.newaxis,:], T, axis=1)\n",
    "\n",
    "    # Exponential discount factor\n",
    "    Gamma = np.array([gamma**i for i in range(T)]).reshape(1,-1)\n",
    "    n_steps_r = (Gamma*rewards_repeated*episode_mask*n_steps_mask_b).sum(axis=2)/Gamma\n",
    "    return n_steps_r, episode_mask, n_steps_mask_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.36133623, 1.33932781, 2.34680569, 1.21575236, 2.35999632],\n",
       "       [1.23633766, 1.3441546 , 1.27875113, 1.34319746, 2.33246183]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_rewards = rewards + entropy.detach().numpy()\n",
    "np_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps_r:  [[4.98737502 3.66266545 2.34680569 3.55214872 2.35999632]\n",
      " [3.8203547  3.92658606 4.89456247 3.65233468 2.33246183]]\n",
      "episode_mask:  [[[1 1 1 0 0]\n",
      "  [1 1 1 0 0]\n",
      "  [1 1 1 0 0]\n",
      "  [0 0 0 1 1]\n",
      "  [0 0 0 1 1]]\n",
      "\n",
      " [[1 1 1 1 1]\n",
      "  [1 1 1 1 1]\n",
      "  [1 1 1 1 1]\n",
      "  [1 1 1 1 1]\n",
      "  [1 1 1 1 1]]]\n",
      "n_steps_mask_b:  [[[1 1 1 0 0]\n",
      "  [0 1 1 1 0]\n",
      "  [0 0 1 1 1]\n",
      "  [0 0 0 1 1]\n",
      "  [0 0 0 0 1]]\n",
      "\n",
      " [[1 1 1 0 0]\n",
      "  [0 1 1 1 0]\n",
      "  [0 0 1 1 1]\n",
      "  [0 0 0 1 1]\n",
      "  [0 0 0 0 1]]]\n"
     ]
    }
   ],
   "source": [
    "n_steps_r, episode_mask, n_steps_mask_b = numpy_check(np_rewards, done)\n",
    "print(\"n_steps_r: \", n_steps_r)\n",
    "print(\"episode_mask: \", episode_mask)\n",
    "print(\"n_steps_mask_b: \", n_steps_mask_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.98737498, 3.66266537, 2.34680569, 3.55214874, 2.35999632],\n",
       "       [3.82035468, 3.92658601, 4.8945626 , 3.6523347 , 2.33246183]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_n_steps_r.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.98737502, 3.66266545, 2.34680569, 3.55214872, 2.35999632],\n",
       "       [3.8203547 , 3.92658606, 4.89456247, 3.65233468, 2.33246183]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some precision issue, but except for that the result is nearly identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
