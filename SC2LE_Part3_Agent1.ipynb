{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StarCraft II : testing the first agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from RelationalModule import CoordActorCritic\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from pysc2.env import sc2_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_game(interface_dict, max_steps_per_episode=1000, **kwargs):\n",
    "    \n",
    "    race = sc2_env.Race(1) # 1 = terran\n",
    "    agent = sc2_env.Agent(race, \"Testv0\") # NamedTuple [race, agent_name]\n",
    "    agent_interface_format = env.parse_agent_interface_format(**interface_dict) #AgentInterfaceFormat instance\n",
    "\n",
    "    game_params = dict(map_name='MoveToBeacon', # simplest minigame\n",
    "                       players=[agent], # use a list even for single player\n",
    "                       game_steps_per_episode = max_steps_per_episode,\n",
    "                       agent_interface_format=[agent_interface_format] # use a list even for single player\n",
    "                       )  \n",
    "    env = sc2_env.SC2Env(**game_params, **kwargs)\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs):\n",
    "    player_relative = obs[0].observation['feature_screen'][_PLAYER_RELATIVE]\n",
    "\n",
    "    player_y, player_x = (player_relative == _PLAYER_FRIENDLY).nonzero()\n",
    "    player_pos = [player_x.mean(), player_y.mean()]\n",
    "\n",
    "    beacon_ys, beacon_xs = (player_relative == _PLAYER_NEUTRAL).nonzero()\n",
    "    if beacon_ys.any():\n",
    "        beacon_pos = [beacon_xs.mean(), beacon_ys.mean()]\n",
    "    else:\n",
    "        beacon_pos = [-1., -1.]\n",
    "\n",
    "    beacon_exists = float(beacon_ys.any())\n",
    "\n",
    "    selected = obs[0].observation['feature_screen'][_SELECTED]\n",
    "    is_selected = np.any((selected==1).nonzero()[0]).astype(float) \n",
    "\n",
    "    state = np.concatenate([player_pos, beacon_pos, [beacon_exists, is_selected]])\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(agent, env, max_steps):\n",
    "\n",
    "    # Start the episode\n",
    "    obs = env.reset()\n",
    "    state = get_state(obs)\n",
    "    \n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    distributions = []\n",
    "    states = [state]\n",
    "    done = []\n",
    "    bootstrap = []\n",
    "        \n",
    "    steps = 0\n",
    "    while True:\n",
    "     \n",
    "        action, log_prob, distrib = agent.step(obs, return_log = True)\n",
    "        new_state, reward, terminal, info = env.step(action)\n",
    "        if debug: print(\"state.shape: \", new_state.shape)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        distributions.append(distrib)\n",
    "        states.append(new_state)\n",
    "        done.append(terminal)\n",
    "        \n",
    "        # Still unclear how to retrieve max steps from the game itself\n",
    "        if terminal is True and steps == max_steps:\n",
    "            bootstrap.append(True)\n",
    "        else:\n",
    "            bootstrap.append(False) \n",
    "        \n",
    "        if terminal is True:\n",
    "            #print(\"steps: \", steps)\n",
    "            #print(\"Bootstrap needed: \", bootstrap[-1])\n",
    "            break\n",
    "            \n",
    "        state = new_state\n",
    "        steps += 1\n",
    "        \n",
    "    rewards = np.array(rewards)\n",
    "    states = np.array(states)\n",
    "    if debug: print(\"states.shape: \", states.shape)\n",
    "    done = np.array(done)\n",
    "    bootstrap = np.array(bootstrap)\n",
    "\n",
    "    return rewards, log_probs, distributions, np.array(states), done, bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_dict = dict(save_replay_episodes=100,\n",
    "                   replay_dir='Replays/',\n",
    "                   replay_prefix='Agent1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
