{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from importlib import reload\n",
    "from RelationalModule import MLP_AC_networks as net\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.lib import actions\n",
    "from pysc2.lib import features\n",
    "from pysc2.env import sc2_env, run_loop, available_actions_printer\n",
    "from pysc2 import maps\n",
    "from absl import flags\n",
    "\n",
    "# indexes of useful layers of the screen_features\n",
    "_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index \n",
    "_SELECTED = features.SCREEN_FEATURES.selected.index\n",
    "\n",
    "# Identifiers in player_relative feature layer\n",
    "_BACKGROUND = 0\n",
    "_PLAYER_FRIENDLY = 1\n",
    "_PLAYER_ALLIES = 2\n",
    "_PLAYER_NEUTRAL = 3\n",
    "_PLAYER_HOSTILE = 4\n",
    "\n",
    "# Ids of the actions that we'll use\n",
    "_NO_OP = actions.FUNCTIONS.no_op.id\n",
    "_MOVE_SCREEN = actions.FUNCTIONS.Attack_screen.id\n",
    "_SELECT_ARMY = actions.FUNCTIONS.select_army.id\n",
    "\n",
    "# Meaning of some arguments required by the actions\n",
    "_SELECT_ALL = [0]\n",
    "_NOT_QUEUED = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "race = sc2_env.Race(1) # 1 = terran\n",
    "agent = sc2_env.Agent(race, \"Testv0\") # NamedTuple [race, agent_name]\n",
    "\n",
    "interface_dict = dict(feature_screen=16, # screen resolution in pixel\n",
    "                      feature_minimap=16, # minimap resolution in pixel (smaller or equal to screen)\n",
    "                      action_space=\"FEATURES\") # either FEATURES or RGB - suggested: FEATURES\n",
    "\n",
    "agent_interface_format = sc2_env.parse_agent_interface_format(**interface_dict) #AgentInterfaceFormat instance\n",
    "\n",
    "game_params = dict(map_name='MoveToBeacon', # simplest minigame\n",
    "                   players=[agent], # use a list even for single player\n",
    "                   agent_interface_format=[agent_interface_format] # use a list even for single player\n",
    "                   )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an envirnoment\n",
    "env = sc2_env.SC2Env(**game_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an high-level state\n",
    "\n",
    "For sure the most relevant informations are the position of the marine and the one of the center of the beacon. Then it might be useful to have a boolean feature telling us whether the beacon exists of not in the map and finally another flag telling us if the marine is selected or not (so that instead of relying only on the final mask for the available actions we can learn that some action, e.g. select army, are more valuable if we don't have units selected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs):\n",
    "    player_relative = obs[0].observation['feature_screen'][_PLAYER_RELATIVE]\n",
    "    \n",
    "    player_y, player_x = (player_relative == _PLAYER_FRIENDLY).nonzero()\n",
    "    player_pos = [player_x.mean(), player_y.mean()]\n",
    "\n",
    "    beacon_ys, beacon_xs = (player_relative == _PLAYER_NEUTRAL).nonzero()\n",
    "    if beacon_ys.any():\n",
    "        beacon_pos = [beacon_xs.mean(), beacon_ys.mean()]\n",
    "    else:\n",
    "        beacon_pos = [-1., -1.]\n",
    "        \n",
    "    beacon_exists = float(beacon_ys.any())\n",
    "    \n",
    "    selected = obs[0].observation['feature_screen'][_SELECTED]\n",
    "    is_selected = np.any((selected==1).nonzero()[0]).astype(float) \n",
    "    \n",
    "    state = np.concatenate([player_pos, beacon_pos, [beacon_exists, is_selected]])\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NamedNumpyArray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n",
       "                 [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "                [None, None],\n",
       "                dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_relative = obs[0].observation['feature_screen'][_PLAYER_RELATIVE]\n",
    "player_relative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read:\n",
    "- 0 stays for background cells\n",
    "- 1 for cells owned by friendly units and buildings\n",
    "- 3 for cells occupied by neutral units or objects (the beacon in our case)\n",
    "\n",
    "Observation: interestingly enough, a map of 16 by 16 is good enough to represent both our unit and the beacon, so there is no need to consider greater resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NamedNumpyArray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "                [None, None],\n",
       "                dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected = obs[0].observation['feature_screen'][_SELECTED]\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_selected = np.any((selected==1).nonzero()[0]).astype(float) \n",
    "is_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, no unit selected at the beginning..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = actions.FunctionCall(_SELECT_ARMY, [_SELECT_ALL])\n",
    "new_obs = env.step(actions=[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NamedNumpyArray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "                [None, None],\n",
       "                dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected = new_obs[0].observation['feature_screen'][_SELECTED]\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_selected = np.any((selected==1).nonzero()[0]).astype(float) \n",
    "is_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "\n",
    "Inspecting the action specifics for this map, one can see that actually all the actions are listed, so that is not going to help in defining the action space. Moreover the available actions change w.r.t. the state, that basically is whether our unit is selected or not.\n",
    "\n",
    "Reduce the action space to 3 moves:\n",
    "1. _NO_OP\n",
    "2. _SELECT_ARMY\n",
    "3. _MOVE_SCREEN\n",
    "\n",
    "Move screen is the only one that can be unavailable if the agent is not selected.\n",
    "\n",
    "We are going to compute a custom mask starting from the available actions and the ids of these 3 actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scripted_arguments(action_id, obs):\n",
    "    \n",
    "    if action_id == _SELECT_ARMY:\n",
    "        args = [_SELECT_ALL]\n",
    "        \n",
    "    elif action_id == _MOVE_SCREEN:\n",
    "        player_relative = obs[0].observation['feature_screen'][_PLAYER_RELATIVE]\n",
    "    \n",
    "        player_y, player_x = (player_relative == _PLAYER_FRIENDLY).nonzero()\n",
    "        player_pos = [int(player_x.mean()), int(player_y.mean())]\n",
    "\n",
    "        beacon_ys, beacon_xs = (player_relative == _PLAYER_NEUTRAL).nonzero()\n",
    "        \n",
    "        if beacon_ys.any():\n",
    "            coord = [int(beacon_xs.mean()), int(beacon_ys.mean())]\n",
    "        else:\n",
    "            coord = player_pos\n",
    "            \n",
    "        args = [coord]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        args = []\n",
    "        \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'RelationalModule.MLP_AC_networks' from '/home/nicola/Nicola_unipd/MasterThesis/SC2-RL/RelationalModule/MLP_AC_networks.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = 3\n",
    "observation_space = 6\n",
    "actor = net.Actor(action_space, observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  [ 8.  3. 11.  9.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "state = get_state(obs)\n",
    "print(\"state: \", state)\n",
    "state = torch.tensor(state).float()\n",
    "\n",
    "aa = obs[0].observation.available_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.7379, -0.1935,    -inf], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs = actor(state, aa)\n",
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  1\n"
     ]
    }
   ],
   "source": [
    "probs = torch.exp(log_probs)\n",
    "distribution = Categorical(probs)\n",
    "a = distribution.sample().item()\n",
    "print(\"action: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_id:  _Functions.select_army\n"
     ]
    }
   ],
   "source": [
    "action_id = actor.action_dict[a]\n",
    "print(\"action_id: \", action_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args:  [[0]]\n"
     ]
    }
   ],
   "source": [
    "args = get_scripted_arguments(action_id, obs)\n",
    "print(\"args: \", args)\n",
    "action = actions.FunctionCall(action_id, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.step([action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.,  3., 11.,  9.,  1.,  1.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = get_state(obs)\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all this part should be included in the get_action method of the actor critic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a one-hot-encoded version of the most important layers\n",
    "\n",
    "Given the fact that our agent was able to achieve top results from the high-level representation of the state, we now know that the layers `player_relative` and `selected` contain all the information that we need to solve the task. So the idea now is to use encode them as one-hot layers (selected layer is already binary, so doesn't need any elaboration, whereas `player_relative` has 2 values to encode, 1 and 3). This will result in a state of shape (3, map_size, map_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ohe_state(obs):\n",
    "    \n",
    "    player_relative = obs[0].observation['feature_screen'][_PLAYER_RELATIVE]\n",
    "    selected = obs[0].observation['feature_screen'][_SELECTED].astype(float)\n",
    "    \n",
    "    friendly = (player_relative == _PLAYER_FRIENDLY).astype(float)\n",
    "    neutral = (player_relative == _PLAYER_NEUTRAL).astype(float)\n",
    "    \n",
    "    state = np.zeros((3,)+player_relative.shape).astype(float)\n",
    "    state[0] = friendly\n",
    "    state[1] = neutral\n",
    "    state[2] = selected\n",
    "    \n",
    "    \n",
    "    print('player_relative: ',player_relative.shape)\n",
    "    print('selected: ', selected.shape)\n",
    "    print('player_relative: ',player_relative)\n",
    "    print('selected: ', selected)\n",
    "    print(\"state.shape: \", state.shape)\n",
    "    print(\"state: \", state)\n",
    "       \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_relative:  (16, 16)\n",
      "selected:  (16, 16)\n",
      "player_relative:  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 3 3 3 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 3 3 3 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 3 3 3 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "selected:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "state.shape:  (3, 16, 16)\n",
      "state:  [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "state = get_ohe_state(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = actions.FunctionCall(_SELECT_ARMY, [_SELECT_ALL])\n",
    "new_obs = env.step(actions=[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_relative:  (16, 16)\n",
      "selected:  (16, 16)\n",
      "player_relative:  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 3 3 3 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 3 3 3 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 3 3 3 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "selected:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "state.shape:  (3, 16, 16)\n",
      "state:  [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "new_state = get_ohe_state(new_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling arguments for the actions\n",
    "\n",
    "Suppose we know which action the agent has chosen and we need to choose also the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start from the simplest one\n",
    "action_id = _NO_OP\n",
    "# In this wayy we can see how many arguments that action accepts\n",
    "print(len(actions.FUNCTIONS[action_id].args))\n",
    "actions.FUNCTIONS[action_id].args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actions.FUNCTIONS[action_id].args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ArgumentType(id=7, name='select_add', sizes=(2,), fn=<function ArgumentType.enum.<locals>.factory.<locals>.<lambda> at 0x7f619c423ef0>, values=<enum 'SelectAdd'>, count=None)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_id = _SELECT_ARMY\n",
    "print(len(actions.FUNCTIONS[action_id].args))\n",
    "actions.FUNCTIONS[action_id].args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_id = _MOVE_SCREEN\n",
    "print(len(actions.FUNCTIONS[action_id].args))\n",
    "actions.FUNCTIONS[action_id].args[1].sizes # not used -> does not load screen resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = new_obs[0].observation.available_actions # not used\n",
    "len(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "13\n",
      "573\n"
     ]
    }
   ],
   "source": [
    "print(len(env.action_spec()))\n",
    "print(len(env.action_spec()[0]))\n",
    "print(len(env.action_spec()[0][0])) # arguments ?\n",
    "print(len(env.action_spec()[0][1])) # actions id ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions = env.action_spec()[0][1]\n",
    "all_arguments = env.action_spec()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/Attack_screen (3/queued [2]; 0/screen [16, 16])\n",
      "(ArgumentType(id=3, name='queued', sizes=(2,), fn=None, values=None, count=None), ArgumentType(id=0, name='screen', sizes=(16, 16), fn=None, values=None, count=None))\n",
      "3/queued [2]\n",
      "(2,)\n",
      "0/screen [16, 16]\n",
      "(16, 16)\n"
     ]
    }
   ],
   "source": [
    "print(all_actions[action_id]) # ok\n",
    "print(all_actions[action_id].args)\n",
    "print(all_actions[action_id].args[0])\n",
    "print(all_actions[action_id].args[0].sizes)\n",
    "print(all_actions[action_id].args[1])\n",
    "print(all_actions[action_id].args[1].sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screen\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(all_actions[action_id].args[1].name)\n",
    "print(all_actions[action_id].args[1].id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more complete way to deal with sampling arguments would be to:\n",
    "1. Learn a representation of the action id and concatenate it somehow to the state representation\n",
    "2. For each argument type have a specific net that samples the arguments\n",
    "\n",
    "So for example, since the action-state representation has always the same shape, all actions that require an argument of type 0 (screen) will be processed by that specific net. Basically since each argument has a completely different meaning, there is little to none transfer between certain arguments (at least in the simplest version). If two parameters are always sampled together and affect each other outcome could be more sensible to consider them as a single argument maybe (I'm thinking about drawing rectangles, in which you have to choose two points in a map and they of course influence each other...).\n",
    "\n",
    "Keeping it as simple as possible, at the moment we have the following actions and arguments type:\n",
    "1. _NO_OP (args=[ ])\n",
    "2. _SELECT_ARMY (args=[ 7/select_add [2]])\n",
    "3. _MOVE_SCREEN (args=[3/queued [2], 0/screen [16, 16])\n",
    "\n",
    "So, 2 categorical arguments and a spatial one (composed by 2 numerical arguments, that in general influence one another).\n",
    "\n",
    "Also notice that while in this exact moment every argument is unique so we wouldn't need AT ALL to concatenate also the encoded action to the input we will feed to the argument-net, in the general case the same argument can be shared by more functions, with the same meaning (of the argument, not the function), but different context. So I think it will be a good thing to start already to process and feed also the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArgumentType(id=0, name='screen', sizes=(16, 16), fn=None, values=None, count=None)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_arguments[0] # all is already specified in the action.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/screen [16, 16]\n",
      "1/minimap [16, 16]\n",
      "2/screen2 [16, 16]\n",
      "3/queued [2]\n",
      "4/control_group_act [5]\n",
      "5/control_group_id [10]\n",
      "6/select_point_act [4]\n",
      "7/select_add [2]\n",
      "8/select_unit_act [4]\n",
      "9/select_unit_id [500]\n",
      "10/select_worker [4]\n",
      "11/build_queue_id [10]\n",
      "12/unload_id [500]\n"
     ]
    }
   ],
   "source": [
    "for arg in all_arguments:\n",
    "    print(arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are 3 arguments that require spatial arguments and all the other are non-spatial (probably categorical).\n",
    "\n",
    "A simple way to discriminate between the two is to take the length of the argument: if it's 2, is spatial, otherwise is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/screen [16, 16]  - len: 2\n",
      "1/minimap [16, 16]  - len: 2\n",
      "2/screen2 [16, 16]  - len: 2\n",
      "3/queued [2]  - len: 1\n",
      "4/control_group_act [5]  - len: 1\n",
      "5/control_group_id [10]  - len: 1\n",
      "6/select_point_act [4]  - len: 1\n",
      "7/select_add [2]  - len: 1\n",
      "8/select_unit_act [4]  - len: 1\n",
      "9/select_unit_id [500]  - len: 1\n",
      "10/select_worker [4]  - len: 1\n",
      "11/build_queue_id [10]  - len: 1\n",
      "12/unload_id [500]  - len: 1\n"
     ]
    }
   ],
   "source": [
    "for arg in all_arguments:\n",
    "    print(arg, ' - len:', len(arg.sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to sample spatial arguments\n",
    "\n",
    "Ideally we would like to extract the x and the y NOT independently. Imagine that there are 2 points of interest: if I have to choose one of them I have 2 combinations, but if I pick x and y independently I will come up with 4 combinations, 2 of them wrong, even though the independent sampling is ideal.\n",
    "So: either we sample one first and then the other, or we sample on the combinations of the two. Of course the latter has a much higher action space (e.g. 256 vs 32 for 16 by 16 map). That's why transposed convolution comes in handy: we can focus on obtaining a smaller-resolution \"value map\", up-sample it the best we can and take the argmax (x and y in once).\n",
    "\n",
    "Instead if we're already conserving the spatial resolution of the input through convolution plus padding (kind of residual convolution block), we can just stack another couple of layers like that and then take the argmax (or sample in other ways, that seems a better option for spatial arguments in particular)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 10])\n",
      "torch.Size([1, 100])\n",
      "tensor([[-4.8747, -4.5430, -4.9874, -4.2471, -4.2104, -5.0799, -4.5612, -4.7650,\n",
      "         -5.0269, -4.6800, -4.7378, -4.4639, -4.4746, -5.0720, -4.3175, -4.2630,\n",
      "         -4.9931, -4.6274, -4.9175, -5.0775, -4.7903, -4.1892, -4.2678, -4.1529,\n",
      "         -4.2007, -4.4733, -4.4531, -4.9254, -5.1245, -4.9852, -4.9745, -4.7358,\n",
      "         -4.5624, -4.1850, -4.1564, -4.9362, -4.4152, -5.0416, -4.4424, -4.7724,\n",
      "         -4.4305, -5.1204, -4.5099, -5.0990, -4.4828, -5.1418, -4.8350, -5.1153,\n",
      "         -4.2163, -4.7095, -5.1078, -5.1162, -4.7959, -4.4934, -4.5770, -4.9251,\n",
      "         -5.1126, -4.1996, -4.6481, -4.2109, -4.2490, -5.0890, -5.1175, -5.0531,\n",
      "         -4.5726, -4.5354, -4.5653, -4.5224, -4.5150, -4.9256, -4.5153, -4.2325,\n",
      "         -4.4099, -4.1518, -5.1123, -4.6827, -4.8233, -4.3350, -4.1948, -4.2128,\n",
      "         -4.2642, -5.0720, -4.9558, -4.9797, -4.6882, -4.4460, -5.1393, -4.5437,\n",
      "         -4.9601, -4.8650, -4.4731, -4.2442, -4.5387, -4.6811, -4.2125, -4.4478,\n",
      "         -4.4141, -4.6981, -4.8913, -4.5744]])\n",
      "tensor([[0.0076, 0.0106, 0.0068, 0.0143, 0.0148, 0.0062, 0.0104, 0.0085, 0.0066,\n",
      "         0.0093, 0.0088, 0.0115, 0.0114, 0.0063, 0.0133, 0.0141, 0.0068, 0.0098,\n",
      "         0.0073, 0.0062, 0.0083, 0.0152, 0.0140, 0.0157, 0.0150, 0.0114, 0.0116,\n",
      "         0.0073, 0.0059, 0.0068, 0.0069, 0.0088, 0.0104, 0.0152, 0.0157, 0.0072,\n",
      "         0.0121, 0.0065, 0.0118, 0.0085, 0.0119, 0.0060, 0.0110, 0.0061, 0.0113,\n",
      "         0.0058, 0.0079, 0.0060, 0.0148, 0.0090, 0.0060, 0.0060, 0.0083, 0.0112,\n",
      "         0.0103, 0.0073, 0.0060, 0.0150, 0.0096, 0.0148, 0.0143, 0.0062, 0.0060,\n",
      "         0.0064, 0.0103, 0.0107, 0.0104, 0.0109, 0.0109, 0.0073, 0.0109, 0.0145,\n",
      "         0.0122, 0.0157, 0.0060, 0.0093, 0.0080, 0.0131, 0.0151, 0.0148, 0.0141,\n",
      "         0.0063, 0.0070, 0.0069, 0.0092, 0.0117, 0.0059, 0.0106, 0.0070, 0.0077,\n",
      "         0.0114, 0.0143, 0.0107, 0.0093, 0.0148, 0.0117, 0.0121, 0.0091, 0.0075,\n",
      "         0.0103]])\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "size = 10\n",
    "# let's say this are the logits in output, each corresponding to a spatial argument value\n",
    "x = torch.rand(1,size,size) \n",
    "print(x.shape)\n",
    "x = x.reshape(x.shape[:-2]+(-1,))\n",
    "print(x.shape)\n",
    "log_probs = F.log_softmax(x, dim=(-1))\n",
    "print(log_probs)\n",
    "probs = torch.exp(log_probs)\n",
    "print(probs)\n",
    "print(probs.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2])\n"
     ]
    }
   ],
   "source": [
    "x_lin = torch.arange(size)\n",
    "xx = x_lin.repeat(size,1)\n",
    "args = torch.cat([xx.view(size,size,1), xx.T.view(size,size,1)], axis=2)\n",
    "args = args.reshape(-1,2)\n",
    "print(args.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 7])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution = Categorical(probs)\n",
    "index = distribution.sample().item() # detaching it, is it okay? maybe...\n",
    "args[index] # and this are the sampled coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complication: actor update\n",
    "\n",
    "Without composite actions we had the following loss for each action:\n",
    "$L = -A(s) \\cdot log(\\pi(a|s))$ <br>\n",
    "Now we decompose the action and the policy like this: <br>\n",
    "$a = (a^0,a^1,\\ldots,a^n)$, <br>\n",
    "where $a^0$ is the action identifier and all the others are the arguments; <br>\n",
    "$\\pi(a|s) = \\left(\\prod_{i=1}^{n}\\pi_i(a^i|a^0,s)\\right) \\cdot \\pi_0(a^0|s)$, <br>\n",
    "assuming that all the $a^i, i=1,\\ldots,n$ are sampled independently from each other but conditioned to the sampled $a^0$. \n",
    "\n",
    "Plugging all together inside the previous formula: <br>\n",
    "$L = -A(s) \\cdot \\left(\\sum_{i=1}^{n}log(\\pi_i(a^i|a^0,s)) + log(\\pi_0(a^0|s)))\\right)$\n",
    "\n",
    "If then for semplicity / for the moment we drop also the $a^0$ dependence: <br>\n",
    "$L = -A(s) \\cdot \\sum_{i=0}^{n}log(\\pi_i(a^i|s))$\n",
    "\n",
    "Forgetting about the full entropy regularization (because it becomes more complicated than it's worth), we can simply return the sum of all the logarithms, the one of the action and those of its parameters, and make the AC update as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures\n",
    "\n",
    "The architectures for choosing the action's argument will depend on how the state representation is, array-like or matrix-like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Supplementary material/SC2 architectures.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start considering an array-like representation (that is the one we are using in agent 2 successfully):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 24\n",
    "state_rep = torch.rand(1,n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features, size, hiddens=[32,16]):\n",
    "        super(CategoricalNet, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        layers.append(nn.Linear(n_features, hiddens[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "            \n",
    "        for i in range(0,len(hiddens)-1):\n",
    "            layers.append(nn.Linear(hiddens[i], hiddens[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        layers.append(nn.Linear(hiddens[-1], size))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, state_rep):\n",
    "        logits = self.net(state_rep)\n",
    "        log_probs = F.log_softmax(logits, dim=(-1))\n",
    "        probs = torch.exp(log_probs)\n",
    "        distribution = Categorical(probs)\n",
    "        arg = distribution.sample().item() \n",
    "        return arg, log_probs.view(-1)[arg], probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg:  1\n",
      "log_prob:  tensor(-0.6344, grad_fn=<SelectBackward>)\n",
      "probs.shape;  torch.Size([1, 2])\n",
      "probs.sum();  tensor(1., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "categorical_net = CategoricalNet(24, 2)\n",
    "arg, log_prob, probs = categorical_net(state_rep)\n",
    "print(\"arg: \", arg)\n",
    "print(\"log_prob: \", log_prob)\n",
    "print(\"probs.shape; \", probs.shape)\n",
    "print(\"probs.sum(); \", probs.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True\n",
    "\n",
    "class SpatialNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features, size=[16,16], n_channels=12):\n",
    "        super(SpatialNet, self).__init__()\n",
    "        \n",
    "        self.size = size[0]\n",
    "        \n",
    "        self.linear = nn.Linear(n_features, (size[0]-6)*(size[1]-6))\n",
    "        \n",
    "        self.conv_block = nn.Sequential(\n",
    "                                        nn.ConvTranspose2d(in_channels=1, \n",
    "                                                           out_channels=n_channels, \n",
    "                                                            kernel_size=3),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.ConvTranspose2d(in_channels=n_channels, \n",
    "                                                           out_channels=n_channels, \n",
    "                                                           kernel_size=3),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.ConvTranspose2d(in_channels=n_channels, \n",
    "                                                              out_channels=n_channels, \n",
    "                                                              kernel_size=3)\n",
    "                                        )\n",
    "        \n",
    "    def forward(self, state_rep):\n",
    "        if debug: print(\"state_rep.shape: \", state_rep.shape)\n",
    "            \n",
    "        x = F.relu(self.linear(state_rep))\n",
    "        if debug: print(\"x.shape (after linear): \", x.shape)\n",
    "            \n",
    "        x = x.reshape(x.shape[0], 1, size[0]-6, size[1]-6)\n",
    "        if debug: print(\"x.shape (after reshape): \", x.shape)\n",
    "            \n",
    "        x = self.conv_block(x)\n",
    "        if debug: print(\"x.shape (after conv block): \", x.shape)\n",
    "            \n",
    "        x, _ = torch.max(x, axis=1)\n",
    "        if debug: print(\"x.shape (after maxpool): \", x.shape)\n",
    "            \n",
    "        x = x.reshape(x.shape[:-2]+(-1,))\n",
    "        \n",
    "        log_probs = F.log_softmax(x, dim=(-1))\n",
    "        if debug: \n",
    "            print(\"log_probs.shape: \", log_probs.shape)\n",
    "            print(\"log_probs.shape (reshaped): \", log_probs.view(self.size, self.size).shape)\n",
    "        probs = torch.exp(log_probs)\n",
    "        \n",
    "        # assume squared space\n",
    "        x_lin = torch.arange(self.size)\n",
    "        xx = x_lin.repeat(self.size,1)\n",
    "        args = torch.cat([xx.view(self.size,self.size,1), xx.T.view(self.size,self.size,1)], axis=2)\n",
    "        args = args.reshape(-1,2)\n",
    "        \n",
    "        distribution = Categorical(probs)\n",
    "        index = distribution.sample().item() # detaching it, is it okay? maybe...\n",
    "        arg = args[index] # and this are the sampled coordinates\n",
    "        arg = arg.detach().numpy()\n",
    "        return arg, log_probs.view(self.size, self.size)[arg[0], arg[1]], probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spatial net architecture:**\n",
    "\n",
    "1. **Linear layer + relu:** <br>\n",
    "    (b, n_features) -> (b, (size[0]-6)^2)\n",
    "2. **Reshape for convolution:** <br>\n",
    "    (b, (size[0]-6)^2) -> (b, 1, size[0]-6, size[1]-6)\n",
    "3. **Transpose Convolution 2D + relu:** <br>\n",
    "    (b, 1, size[0]-6, size[1]-6) -> (b, n_channels, size[0]-4, size[1]-4)\n",
    "3. **Transpose Convolution 2D + relu:** <br>\n",
    "    (b, n_channels, size[0]-4, size[1]-4) -> (b, n_channels, size[0]-2, size[1]-2)\n",
    "4. **Transpose Convolution 2D :** <br>\n",
    "    (b, n_channels, size[0]-2, size[1]-2) -> (b, n_channels, size[0], size[1])\n",
    "5. **Pixel-wise max pooling:** <br>\n",
    "    (b, n_channels, size[0], size[1]) -> (b, size[0], size[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_rep.shape:  torch.Size([1, 24])\n",
      "x.shape (after linear):  torch.Size([1, 100])\n",
      "x.shape (after reshape):  torch.Size([1, 1, 10, 10])\n",
      "x.shape (after conv block):  torch.Size([1, 12, 16, 16])\n",
      "x.shape (after maxpool):  torch.Size([1, 16, 16])\n",
      "log_probs.shape:  torch.Size([1, 256])\n",
      "log_probs.shape (reshaped):  torch.Size([16, 16])\n",
      "arg:  [ 8 12]\n",
      "log_prob:  tensor(-5.5477, grad_fn=<SelectBackward>)\n",
      "probs.shape;  torch.Size([1, 256])\n",
      "probs.sum();  tensor(1., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "screen_net = SpatialNet(24)\n",
    "arg, log_prob, probs = screen_net(state_rep)\n",
    "print(\"arg: \", arg)\n",
    "print(\"log_prob: \", log_prob)\n",
    "print(\"probs.shape; \", probs.shape)\n",
    "print(\"probs.sum(); \", probs.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic instantiation of the argument nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info that we need to know beforehand\n",
    "action_dict = {0:_NO_OP, 1:_SELECT_ARMY, 2:_MOVE_SCREEN}\n",
    "all_actions = env.action_spec()[0][1]\n",
    "all_arguments = env.action_spec()[0][0]\n",
    "n_features = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "arg.name:  select_add\n",
      "size:  (2,)\n",
      "Init CategoricalNet for select_add argument\n",
      "\n",
      "arg.name:  queued\n",
      "size:  (2,)\n",
      "Init CategoricalNet for queued argument\n",
      "\n",
      "arg.name:  screen\n",
      "size:  (16, 16)\n",
      "Init SpatialNet for screen argument\n"
     ]
    }
   ],
   "source": [
    "debug = True\n",
    "arguments_networks = {}\n",
    "arguments_dict = {}\n",
    "\n",
    "for a in action_dict:\n",
    "    action = all_actions[action_dict[a]]\n",
    "    args = action.args\n",
    "    \n",
    "    for arg in args:\n",
    "        arguments_dict[arg.name] = arg.id # store 'name':id pairs for future use\n",
    "        print('\\narg.name: ', arg.name)\n",
    "        \n",
    "        size = all_arguments[arg.id].sizes\n",
    "        if debug: print('size: ', size)\n",
    "        if len(size) == 1:\n",
    "            print(\"Init CategoricalNet for \"+arg.name+' argument')\n",
    "            arguments_networks[arg.name] = CategoricalNet(n_features, size[0]) # hiddens as 3rd arg if needed\n",
    "        else:\n",
    "            print(\"Init SpatialNet for \"+arg.name+' argument')\n",
    "            arguments_networks[arg.name] = SpatialNet(n_features, size) # in_channels as 3rd arg if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'select_add': 7, 'queued': 3, 'screen': 0}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arguments_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0.6066]), tensor([0.6278])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.2344)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [torch.rand(1), torch.rand(1)]\n",
    "print(l)\n",
    "torch.stack(l).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
